{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning competition testing some stuff out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ready\n"
     ]
    }
   ],
   "source": [
    "# PATHS\n",
    "DATA_DIR = 'D:/Users/main/Documents/UGent/1ste Master/Machine Learning/data/data'\n",
    "POSE_DIR = 'D:/Users/main/Documents/UGent/1ste Master/Machine Learning/data/data/pose'\n",
    "print(\"Directory ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready!\n"
     ]
    }
   ],
   "source": [
    "# First we load the main libraries we will need\n",
    "\n",
    "# REQUIRED IMPORTS FROM STANDARD PACKAGES\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "\n",
    "from os.path import join as pjoin\n",
    "from glob import glob\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Normally, all libraries are loaded only once, \n",
    "# even if you execute the import code multiple times\n",
    "rng = np.random.RandomState(69)\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport augmentation\n",
    "%aimport features_extraction\n",
    "\n",
    "\n",
    "print(\"Environment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utilities ready\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS FROM THE UTIL LIBRARY PROVIDED BY US\n",
    "\n",
    "import util.submission as S\n",
    "import util.vis as V\n",
    "import util.metrics as M\n",
    "#import util.MovementHands as MH\n",
    "\n",
    "print('utilities ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: problem and data analysis\n",
    "\n",
    "The first thing we do is loading the data and, if necessary, preprocessing it into a form that is useful for machine learning. Some things you may typically have to do are:\n",
    "\n",
    "- **Converting all data to numerical values:** some data may be in the form of textual labels. These should be converted to numerical values, using an encoding that maximally corresponds to the underlying structure of those labels. In particular, if the labels imply some kind of ordering (e.g., 'small, 'medium', 'large'), this ordering should be reflected in the values (e.g., '1', '2', '3'). If there is no such ordering, it is usually better to use an encoding that does **not** imply this, such as a one-hot encoding. You might for example convert the colours 'red', 'green', 'blue' to the vectors '(0,0,1)', '(0,1,0)', '(1,0,0)'. This effectively replaces one textual feature by three numerical features, but leaves more freedom for the model because it does not have to 'undo' an ordering in the features that was not really there to begin with\n",
    "- **Correcting for missing data:** often, some features are missing for some data samples. These may be indicated as NaN, or '-' or any other non-numerical value. In order to be processed by a machine learning model, such samples can be omited (if there are few), but it is usually better to replace those values by a well-chosen numerical value. Which value makes sense depends very much on the problem at hand and the typical ranges for the feature in question.\n",
    "- **Outlier removal:** generally, it is advised to be very careful with this. Keeping outliers in your data often makes the final model more robust. Remove outliers **only** when real exceptional cases occur in your data **and** they can be expected to hamper the training of your model. The question that should be asked here is whether or not you want your model to be able to deal with such cases. When your model is operational, often extreme cases could be detected upfront and treated differently (i.e., not by your model). Those are the ones you want to leave out of your data set. Any other case is best left in. You might consider to cap extreme values, by replacing them by the maximal (or minimal) value you find reasonable.\n",
    "\n",
    "Obviously, more advanced forms of preprocessing can also be considered, such as filtering or advanced feature extraction. Those will not be discussed in this notebook.\n",
    "\n",
    "As mentioned before, for the data set considered here, the data is already nice and clean. We only have to load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "3718 3718\n"
     ]
    }
   ],
   "source": [
    "dataset_file = pjoin(DATA_DIR, 'labels.csv')\n",
    "\n",
    "all_samples = []\n",
    "all_labels = []\n",
    "all_persons = []\n",
    "all_personlabels = []\n",
    "\n",
    "with open(dataset_file) as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader) # Skips the first row, which is the header\n",
    "    for row in reader:\n",
    "        name, _gloss, label, _person = row\n",
    "        sample = np.load(pjoin(POSE_DIR, 'train', name + '.npy'))\n",
    "        all_samples.append(sample)\n",
    "        all_labels.append(label)\n",
    "        all_persons.append(_person)\n",
    "        all_personlabels.append((label, _person))\n",
    "        \n",
    "all_labels = np.array(all_labels)\n",
    "all_persons = np.array(all_persons)\n",
    "#print(all_persons)\n",
    "print(\"done\")\n",
    "print(len(all_samples), len(all_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[414, 355, 240, 248, 225, 234, 214, 188, 193, 178, 182, 172, 174, 144, 144, 157, 125, 131]\n",
      "125\n",
      "[125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125]\n",
      "2250 2250 2250\n",
      "(10, 137, 3)\n"
     ]
    }
   ],
   "source": [
    "def downsample(all_samples, all_labels, all_persons):\n",
    "    \"\"\"\n",
    "    Functions that samples down untill every label has the same amount of samples\n",
    "    \n",
    "    :param all_samples: all the samples that need to be sampled down\n",
    "    :param all_labels: the corresponding labels with the samples    \n",
    "    \"\"\"\n",
    "    \n",
    "    tot_label_list = [0 for i in range(18)]\n",
    "    for label in all_labels:\n",
    "        tot_label_list[int(label)] += 1\n",
    "    max_labels = min(tot_label_list)\n",
    "    print(tot_label_list)\n",
    "    print(max_labels)\n",
    "    new_samples = []\n",
    "    new_labels = []\n",
    "    new_persons = []\n",
    "    new_labellist = [0 for i in range(18)]\n",
    "    for i in range(len(all_samples)):\n",
    "        sample = all_samples[i]\n",
    "        if (new_labellist[int(all_labels[i])] < max_labels):\n",
    "            new_samples.append(all_samples[i])\n",
    "            new_labels.append(all_labels[i])\n",
    "            new_labellist[int(all_labels[i])] += 1\n",
    "            new_persons.append(all_persons[i])\n",
    "    return new_samples, new_labels, new_labellist, all_persons\n",
    "down_samples, down_labels, down_labellist = downsample(all_samples, all_labels)\n",
    "print(down_labellist)\n",
    "print(len(down_samples), 18 * 125, len(down_labels))\n",
    "print(down_samples[0].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angles(sample, q0, q1, key1, key2,i , shape, control = 1): #control keypoint is the neck keypoint = 1\n",
    "    \"\"\"\n",
    "    Function that calculates the mean angle between two keypoints. The vectors used for the calculations are respective to the\n",
    "    neck.\n",
    "    \n",
    "    :param sample: the sample that is being featured\n",
    "    :param q0: begin of the quarter frame\n",
    "    :param q1: end of the quarter frame\n",
    "    :param key1: number of the first keypoint\n",
    "    :param key2: number of the second keypoint\n",
    "    :param control: the number of the keypoint that is being used to measure the angles with. this is standard set to the neck which has keypoint number 1.\n",
    "    \"\"\"\n",
    "    special_val_angle = math.pi/180*270\n",
    "    ret_list = []\n",
    "    median_angles = []\n",
    "    std_angles = []\n",
    "    angles = np.zeros(q1 - q0)\n",
    "    for j in range(q0, q1): \n",
    "        key1_vector = sample[j][key1] - sample[j][control] #get x and y value\n",
    "        key2_vector = sample[j][key2] - sample[j][control] #get x and y value\n",
    "        if (np.linalg.norm(key1_vector) == 0): #Catch outliers keypoint 1\n",
    "            k = j-1\n",
    "            while (np.linalg.norm(key1_vector) == 0 and k>=0): #previous frames\n",
    "                key1_vector = sample[k][key1] - sample[k][control]\n",
    "                k -=1\n",
    "            if (k == -1 and np.linalg.norm(key1_vector) == 0): #future frames\n",
    "                k = j+1\n",
    "                while (np.linalg.norm(key1_vector) == 0 and k<len(sample)):\n",
    "                    key1_vector = sample[k][key1] - sample[k][control]\n",
    "                    k+=1\n",
    "        if (np.linalg.norm(key2_vector) == 0): #Catch outliers keypoint 2\n",
    "            k = j-1\n",
    "            while (np.linalg.norm(key2_vector) == 0 and k>=0): #previous frames\n",
    "                key2_vector = sample[k][key2] - sample[k][control]\n",
    "                k -=1\n",
    "            if (k == -1 and np.linalg.norm(key2_vector) == 0): #future frames\n",
    "                k = j+1\n",
    "                while (np.linalg.norm(key2_vector) == 0 and k<len(sample)):\n",
    "                    key2_vector = sample[k][key2] - sample[k][control]\n",
    "                    k+=1\n",
    "        key1_vector = key1_vector[:2]/np.linalg.norm(key1_vector[:2])\n",
    "        key2_vector = key2_vector[:2]/np.linalg.norm(key2_vector[:2])\n",
    "        angle = np.arccos(np.clip(np.dot(key1_vector, key2_vector), -1.0, 1.0))\n",
    "        angles[j-q0] = angle\n",
    "    if math.isnan(np.median(angles)):\n",
    "        ret_list.append(special_val_angle)\n",
    "    else:\n",
    "        ret_list.append(np.median(angles))\n",
    "    if math.isnan(np.std(angles)):\n",
    "        ret_list.append(special_val_angle)\n",
    "    else:\n",
    "        ret_list.append(np.std(angles))\n",
    "    \n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_quarters(samplearray, key1, key2, control = 1):\n",
    "    \"\"\"\n",
    "    Function that calculates the angle between the 2 keypoints which used the control keypoint to measure the angles with.\n",
    "    \n",
    "    :param samplearray: the array with all the samples in\n",
    "    :param key1: the first keypoint that is being used\n",
    "    :param key2: the second keypoint that is being used\n",
    "    :param control: the keypoint that is being used to measure the angles against\n",
    "    \"\"\"\n",
    "    samplearray = features_extraction.normalise_frames(samplearray)\n",
    "    angle_list = []\n",
    "    std_list = []\n",
    "    for i in range(len(samplearray)):\n",
    "        sample = samplearray[i]\n",
    "        q0 = 0\n",
    "        q1 = sample.shape[0] // 4\n",
    "        q2 = sample.shape[0] // 2\n",
    "        q3 = sample.shape[0] // 4 * 3\n",
    "        q4 = sample.shape[0]\n",
    "        lijst = [q0, q1, q2, q3, q4]\n",
    "        angle_quarter_list = []\n",
    "        std_quarter_list = []\n",
    "        for quarter in range(1, 5): #use all the quarters\n",
    "            angle, std = angles(sample, lijst[quarter - 1], lijst[quarter], key1, key2,i,sample.shape[0], control)\n",
    "            if (math.isnan(angle) or math.isnan(std)):\n",
    "                print(\"nan value received\")\n",
    "            angle_quarter_list.append(angle)\n",
    "            std_quarter_list.append(std)\n",
    "        angle_list.append(angle_quarter_list)\n",
    "        std_list.append(std_quarter_list)\n",
    "    #print(angle_list)\n",
    "    #print(\"===========Ordering features==========\")\n",
    "    feat_array=[]\n",
    "    feat_array2=[]\n",
    "    for quarter in range(1, 5):\n",
    "        temp=[]\n",
    "        temp2=[]\n",
    "        for i in range(len(angle_list)):\n",
    "            temp.append(angle_list[i][quarter-1])\n",
    "            temp2.append(std_list[i][quarter-1])\n",
    "        feat_array.append(temp)\n",
    "        feat_array2.append(temp2)\n",
    "    #print(feat_array)\n",
    "    #print(\"======================================\")\n",
    "    return feat_array, feat_array2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[0.6501589221264646, 1.5352861318665938, 1.4714456424145443, 1.2839183104241338, 0.42283404690192694, 2.406335513257809, 0.9431900965354353, 1.9852386455513718, 1.720260494930682, 2.801816497895368], [0.41861269536682855, 1.7743666603563297, 1.196720184520906, 1.2411373440241487, 0.47910683347029026, 1.787655865094934, 1.008011810441422, 1.8992385207914597, 1.408256196449348, 2.897086653860201], [0.8670577940009299, 1.5878763278165327, 1.211439432973376, 1.240195377377148, 0.17694002845359177, 1.4235899465745343, 1.0456291777747297, 1.7876927775575144, 1.8174847083552086, 2.9799246569463085], [1.1144753898418096, 1.2732365221442556, 1.3368170544307127, 1.2372072225526278, 0.1489952703399232, 1.6016351648295495, 0.9729589389758923, 1.73863124146451, 1.6534848271558922, 3.115852156519372]], [[0.053509170837717, 0.06306302090376195, 0.060068533938219135, 0.14073187495479306, 0.29369491416211463, 0.2507251924673542, 0.04125890055893134, 0.09318295364924609, 0.21771670434539495, 0.09906306289429612], [0.11370876038246479, 0.10573152522790774, 0.06872020614719189, 0.12552690514521248, 0.05834301298720887, 0.6026048713405525, 0.010279649443300353, 0.06808853747079277, 0.07334323930670993, 0.000732917080068507], [0.0, 0.15705098645189144, 0.03338865832359611, 0.14776739717949344, 0.1062307783495492, 0.15434887927546176, 0.014004231491720911, 0.06191947623030609, 0.04272859578148903, 0.04557866275511557], [0.14694904434846515, 0.12052290445983198, 0.027754824302299114, 0.041447471678544845, 0.8658959528039833, 0.12323100553240454, 0.04825434662606649, 0.06790075937585345, 0.03440423984891326, 0.11542805123923752]])\n",
      "lengtes:  2 4 4 10\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "res = (angle_quarters(all_samples[0:10],4, 7))\n",
    "print(res)\n",
    "for e in range(4):\n",
    "    for i in res[0][e]:\n",
    "        if math.isnan(i):\n",
    "            print(\"angle \", i)\n",
    "    for i in res[1][e]:\n",
    "        if math.isnan(i):\n",
    "            print(\"std \", i)\n",
    "print(\"lengtes: \", len(res), len(res[0]), len(res[1]), len(res[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_personlabels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_labels.shape)\n",
    "print(all_persons.shape)\n",
    "print(all_samples[0].shape)\n",
    "\n",
    "num_keypoints = 137*3\n",
    "num_samples = all_labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, each body part has a different number of keypoints. They are structured in the numpy array as follows:\n",
    "\n",
    "| Part       | Count | Start Index | End Index |\n",
    "|------------|-------|-------------|-----------|\n",
    "| pose       | 25    | 0           | 24        |\n",
    "| face       | 70    | 25          | 94        |\n",
    "| left hand  | 21    | 95          | 115       |\n",
    "| right hand | 21    | 116         | 136       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_left_offset = 25 + 70 # pose + face\n",
    "hand_left_len = 21\n",
    "frame_index = 0\n",
    "x_index = 0\n",
    "\n",
    "hand_left_x = all_samples[0][frame_index, hand_left_offset:hand_left_offset + hand_left_len, x_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "print(\"libraries ready\")\n",
    "\n",
    "plottie = 1117\n",
    "\n",
    "x = [i for i in range(all_samples[plottie].shape[0])]\n",
    "val_pose = 0\n",
    "val_face = 0\n",
    "val_hands = 0\n",
    "val_left = 0\n",
    "val_right = 0\n",
    "pose_values = []\n",
    "face_values = []\n",
    "hands_values = []\n",
    "left_values = []\n",
    "right_values = []\n",
    "\n",
    "\n",
    "for i in range(all_samples[plottie].shape[0]): #get number of frames\n",
    "    avg_pose = sum([all_samples[plottie][i][e][2] for e in range(25)])/25\n",
    "    avg_face = sum([all_samples[plottie][i][e][2] for e in range(25, 95)])/70\n",
    "    avg_hands = sum([all_samples[plottie][i][e][2] for e in range(95, 137)])/42\n",
    "    avg_left = sum([all_samples[plottie][i][e][2] for e in range(95, 116)])/21\n",
    "    avg_right = sum([all_samples[plottie][i][e][2] for e in range(116, 137)])/21\n",
    "\n",
    "    #print(avg_pose, avg_face, avg_hands)\n",
    "    pose_values.append(avg_pose)\n",
    "    face_values.append(avg_face)\n",
    "    hands_values.append(avg_hands)\n",
    "    left_values.append(avg_left)\n",
    "    right_values.append(avg_right)\n",
    "print(\"values extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, pose_values, label = 'Pose')\n",
    "plt.plot(x, face_values, label = 'Face')\n",
    "#plt.plot(x, hands_values, label = 'Hands')\n",
    "plt.plot(x, left_values, label = 'Left Hand')\n",
    "plt.plot(x, right_values, label = 'Right Hand')\n",
    "\n",
    "\n",
    "plt.ylim(0, 1.0)\n",
    "plt.ylabel('Confidence level ')\n",
    "plt.xlabel('Framenumber')\n",
    "plt.title('Confidence levels of keypoints')\n",
    "plt.grid(False)\n",
    "plt.legend()\n",
    "plt.savefig(\"ConfidenceLevels\"+str(plottie)+\".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "print(\"imported\")\n",
    "\n",
    "print(all_persons)\n",
    "print(collections.Counter(all_persons))\n",
    "print(len(collections.Counter(all_persons)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "persons = []\n",
    "for i in all_persons:\n",
    "    if i not in persons:\n",
    "        persons.append(i)\n",
    "num_persons = []\n",
    "for i in persons:\n",
    "    num_persons.append(np.count_nonzero(all_persons == i))\n",
    "    #print(np.count_nonzero(all_persons == i))\n",
    "print(num_persons)\n",
    "print(persons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label_list_all_persons = []\n",
    "for person in persons:\n",
    "    label_list = []\n",
    "    for sample in all_personlabels:\n",
    "        if sample[1] == person:\n",
    "            label_list.append(sample[0])\n",
    "        #label_list.append(np.count_nonzero(all_personlabels[1] == sample[0]))\n",
    "    #print(label_list)\n",
    "    tot_label_list = []\n",
    "    for i in range(18):\n",
    "        tot_label_list.append(0)\n",
    "    for label in label_list:\n",
    "        tot_label_list[int(label)] += 1\n",
    "    #print(tot_label_list)\n",
    "    plt.bar([i for i in range(18)], tot_label_list)\n",
    "    plt.xlabel(\"label id\")\n",
    "    plt.ylabel(\"Amount of videos\")\n",
    "    titel = (\"videos created for person %s\" , person)\n",
    "    plt.title(titel)\n",
    "    #plt.savefig(\"Videos_per_person.png\")\n",
    "    plt.show()\n",
    "    label_list_all_persons.append(tot_label_list)\n",
    "    print(\"person done %s\" % person)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(label_list_all_persons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.bar([i for i in range(18)], tot_label_list)\n",
    "plt.xlabel(\"label id\")\n",
    "plt.ylabel(\"Amount of videos\")\n",
    "plt.title('Videos created per person')\n",
    "#plt.savefig(\"Videos_per_person.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.bar([i for i in range(59)], num_persons)\n",
    "plt.xlabel(\"Person id\")\n",
    "plt.ylabel(\"Amount of videos\")\n",
    "plt.title('Videos created per person')\n",
    "plt.savefig(\"Videos_per_person.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: splitting the data set for validation and testing\n",
    "\n",
    "Machine learning models easily overfit to the training data. The result of overfitting is that the model doesn't generalise well, i.e., the model performance on unseen data is considerably worse than on the training data. For this reason, it is important to try and make a decent estimation of the performance on unseen data by splitting off a test set that is **not used at all** for building the model. \n",
    "\n",
    "We make the distinction between **training** and **tuning the hyperparameters**. In, e.g. , linear regression or logistic regression, training means optimising the weights for a given combination of model and training data. The **hyperparameters** are parameters at a higher level than the weights: they include things like which features (and how many) are used, the regularisation parameter values, or generally anything that affects the model complexity. \n",
    "\n",
    "The best way to look at this is that each setting of hyperparameters corresponds to a different model and from all these possible settings, we want to pick the best or at least a decent one. Again, a 'good' model is a model that performs well on **unseen** data, so we need to select our best hyperparameters based on data we haven't used for training. However, as soon as data has been used for making **any** decision about the model, it is no longer unseen, so we always need to keep some dat apart for evaluation the final quality of our model. \n",
    "\n",
    "Tuning many hyperparameters can also lead to overfitting. If we choose hyperparameters based on the test data, the score on this part of the data will again not be representative anymore for the score on completely unseen data. Therefore, we can not do the hyperparameter selection based on the test data. This is why we need to split the remaining data a second time into a **training set** and a **validation set**. \n",
    "\n",
    "If your test set is too small, your estimate of the generalisation performance will not be reliable, if your validation set is too small, your selection of hyperparameters will be sub-optimal because it's based on an unreliable estimate, and if you make both of these large, you don't have enough data left to train your model. If you have a lot of data, relative to the task and model complexity, this is no problem. However, in most cases, you don't have that amount of data. This is the reason why we often use **cross-validation**: we split the data and optimise the model multiple times and average the model performance accross all runs to get a better estimate of the true validation score. Obviously, the amount of averaging you can afford greatly depends on the training time of each model and the computational resources you have at your disposal. In most cases, k-fold cross-validation is used. Here, the training data is split into k equal parts (called **folds**) and the model is trained k times, each time using a different part as the validation set. The cross-validation score is the average model performance accross all k trained models, each evaluated on their validation set. Once the optimal hyperparameters are chosen, the model is retrained with **all** training data and then tested on the test set.\n",
    "\n",
    "In summary, the usual flow is:\n",
    "\n",
    "- **Split off a test set:** don't look at this data except for the evaluation of your final model\n",
    "- **Decide how many folds you need:** For large data sets, a single validation set is enough to give a decent estimate of the model error. However, for small data sets, the validation score can fluctuate a lot for different splits of the data, especially since we want to use as much data as possible for training, so validation sets will be small. For this reason, the same data set is often split multiple times into a train set and a validation set. For each **fold**, the model is trained and the validation error calculated. The best model parameters are chosen based on the average of the validation errors accross all the folds. One of the most common approaches is called **k-fold cross validation**. Here the dat is split into k equal (non-overlapping) parts and in each fold, a different part of the data is used for validation. \n",
    "- **Optimise the hyperparameters**\n",
    "- **Train your final model:** Once the optimal hyperparameters are found, these can be fixed and a final model can be trained using all the original training data, i.e., all data that was previously used for training **or** validation. This is often forgotten!\n",
    "- **Evaluate your final model:** use the test set you split off for evaluation. If you also need a more reliable test score (because you don't have enough data and have to keep your test set small), you can do nested cross validation by also considering multiple test sets, repeating the whole optimisation procedure for each test set and averaging the test scores. \n",
    "\n",
    "Although cross validation is well supported in sklearn and therefore sounds quite easy, this is also where many mistakes occur. In many real life situations, the data you have available for training is not really \"i.i.d.\". Instead, it comes from different subgroups, for instance, it may have been measured on different days or in different times of the year, originate from different test subjects, be collected by different enqueteurs, come from different customers or companies, ... and in most cases each of these subsets will have (hopefully slightly) different underlying distributions. Also, in most cases, you want your model to be robust against that, for instance, it should work with new patients or customers, or be useful to new companies. In that case, this is exactly what you need to evaluate during cross validation, by keeping entire subgroups within one fold and/or splitting off an entire subgroup in each test set. In addition, ideally, you are involved in the data collection: think about which differences may occur accross subgroups and really push having enough subgroups to allow your model to generalise beyond this. As we will see later in this course, the i.i.d. assumption also doesn't hold for time series. You can find more about cross-validation options on http://scikit-learn.org/stable/modules/cross_validation.html the sklearn documentation page.  \n",
    "\n",
    "Now back to our cancer data set. In this case, we have no additional information about possible subgroups in the data (e.g., the data might have been collected in different labs, by different lab assistants, or with slightly different measurement equipment). Therefore, the best we can do is randomly splitting the data into train and test sets and use cross-validation with random folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test set (25% of total samples)\n",
    "#Train set (75% of total samples)\n",
    "#Validation set(K-fold cross validation of the training set, K=3 to start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mirror(sample):\n",
    "#    mir = np.arange(sample.shape[0]*137*3).reshape(sample.shape[0], 137, 3) #create an empty copy\n",
    "    mir=np.zeros((sample.shape[0],137,3))\n",
    "\n",
    "    for i in range(sample.shape[0]):\n",
    "        for e in range(137):\n",
    "            mir[i][e][0] = float(455) - sample[i][e][0] #mirror the x-coordinates cause input images consist of 455 by 256 pixels\n",
    "            mir[i][e][1] = sample[i][e][1]\n",
    "            mir[i][e][2] = sample[i][e][2]\n",
    "            \n",
    "    return mir #return the mirrored array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_test_split(all_persons, all_personlabels):\n",
    "    \"\"\"\n",
    "    Function that return some variables that help to split the data.\n",
    "    \n",
    "    :return return a person array and a array with how many samples each person has made for each label    \n",
    "    \"\"\"\n",
    "    persons = []\n",
    "    for i in all_persons:\n",
    "        if i not in persons:\n",
    "            persons.append(i)\n",
    "    num_persons = []\n",
    "    for i in persons:\n",
    "        num_persons.append(np.count_nonzero(all_persons == i))\n",
    "        \n",
    "        \n",
    "    label_list_all_persons = []\n",
    "    for person in persons:\n",
    "        label_list = []\n",
    "        for sample in all_personlabels:\n",
    "            if sample[1] == person:\n",
    "                label_list.append(sample[0])\n",
    "        tot_label_list = []\n",
    "        for i in range(18):\n",
    "            tot_label_list.append(0)\n",
    "        for label in label_list:\n",
    "            tot_label_list[int(label)] += 1\n",
    "        label_list_all_persons.append(tot_label_list)\n",
    "        \n",
    "    return persons, label_list_all_persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tot_label_list(all_persons, all_personlabels):\n",
    "    \"\"\"\n",
    "    Get the total_label_list that is being used when upsampling your data\n",
    "    \n",
    "    :return return the tot_label_list that is being used when you want to upsample your data\n",
    "    \n",
    "    \"\"\"\n",
    "    persons, label_list_all_persons = prep_test_split(all_persons, all_personlabels)\n",
    "    tot_label_list = []\n",
    "    for i in range(18):\n",
    "        tot_label_list.append(0)\n",
    "    for labellist in label_list_all_persons:\n",
    "        for i in range(18):\n",
    "            tot_label_list[i] += labellist[i]\n",
    "    return tot_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_help(all_samples, all_labels, all_persons, all_personlabels):\n",
    "    \"\"\"\n",
    "    HELP function for the total upsampling function. This function checks whether or not upsampling is needed for\n",
    "    samples with a low amount of frames\n",
    "    Samples the labels with the lowest amount up to the number of labels from the most common type\n",
    "    \n",
    "    :param all_samples: samplearray\n",
    "    :param all_labels:\n",
    "    :return an array with the total new samples, new labels and a new tot_label_list\n",
    "    \"\"\"\n",
    "    tot_label_list = get_tot_label_list()\n",
    "    upsampled_samples = [sample for sample in all_samples]\n",
    "    upsampled_labels = [label for label in all_labels]\n",
    "    print(len(all_samples), len(all_labels))\n",
    "    print(len(upsampled_samples), len(upsampled_labels))\n",
    "    max_labels = max(tot_label_list)\n",
    "    cur_label_list = [tot_label_list[e] for e in range(18)] #make a list that counts the amount of labels\n",
    "    \n",
    "    for i in range(len(all_samples)):\n",
    "        if (cur_label_list[int(all_labels[i])] < max_labels): #if there are not enough samples yet for that label\n",
    "            cur_label_list[int(all_labels[i])] += 1 #count the label\n",
    "            \n",
    "            sample = all_samples[i]\n",
    "            if (i%2 == 0):\n",
    "                extra_sample_shift = augmentation.move_left_hand(sample, -11, -10)\n",
    "            else:\n",
    "                extra_sample_shift = augmentation.move_left_hand(sample, 17, 15)   \n",
    "            upsampled_samples.append(extra_sample_shift)\n",
    "            upsampled_labels.append(all_labels[i])\n",
    "    for i in range(len(all_samples)):\n",
    "        if (cur_label_list[int(all_labels[i])] < max_labels): #if there are not enough samples yet for that label\n",
    "            cur_label_list[int(all_labels[i])] += 1 #count the label\n",
    "            \n",
    "            sample = all_samples[i]\n",
    "            if (i%2 == 0):\n",
    "                extra_sample_shift = augmentation.move_right_hand(sample, 18, 13)\n",
    "            else:\n",
    "                extra_sample_shift = augmentation.move_right_hand(sample, -16, -18)      \n",
    "            upsampled_samples.append(extra_sample_shift)\n",
    "            upsampled_labels.append(all_labels[i])\n",
    "    return (upsampled_samples, upsampled_labels, cur_label_list)\n",
    "\n",
    "upsampled_samples, upsampled_labels, upsampled_label_list = upsample_help(all_samples, all_labels)\n",
    "print(upsampled_label_list)\n",
    "print(len(upsampled_samples), len(all_samples), len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(all_samples, all_labels, all_persons, all_personlabels):\n",
    "    \"\"\"\n",
    "    This function upsampled the lowest labels and when all the labels are even it does augmentation on the new sampleset\n",
    "    \"\"\"\n",
    "    upsampled_samples, upsampled_labels, upsampled_label_list = upsample_help(all_samples, all_labels)#upsampling the lowest ones\n",
    "    print(len(upsampled_samples), len(all_samples), len(all_labels))\n",
    "    for i in range(len(upsampled_samples)): #add the scaling to every sample too\n",
    "        upsampled_samples.append(augmentation.scale_person(sample, 1.56))\n",
    "        upsampled_labels.append(upsampled_labels[i])\n",
    "        upsampled_label_list[int(upsampled_labels[i])] += 1\n",
    "    return (upsampled_samples, upsampled_labels, upsampled_label_list)\n",
    "upsampled_samples, upsampled_labels, upsampled_label_list = upsample(all_samples, all_labels, all_persons, all_personlabels)\n",
    "print(upsampled_label_list)\n",
    "print(len(upsampled_samples), len(all_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_training(all_samples, all_labels, tot_label_list):\n",
    "    new_training_samples = []\n",
    "    new_training_labels = []\n",
    "    max_labels = min(tot_label_list)\n",
    "    cur_label_list = [0 for e in range(18)] #make a list that counts the amount of labels\n",
    "    \n",
    "    for i in range(len(all_labels)):\n",
    "        if (cur_label_list[int(all_labels[i])] < max_labels): #if there are not enough samples yet for that label\n",
    "            new_training_samples.append(all_samples[i]) ##add the data\n",
    "            new_training_labels.append(all_labels[i]) #add the labels\n",
    "            cur_label_list[int(all_labels[i])] += 1 #count the label\n",
    "            \n",
    "    new_training_samples = np.array(new_training_samples)\n",
    "    new_training_labels = np.array(new_training_labels)\n",
    "    return (new_training_samples, new_training_labels)\n",
    "\n",
    "balanced_samples, balanced_labels = balance_training(upsampled_samples, upsampled_labels, upsampled_label_list) #with upsampling\n",
    "#balanced_samples, balanced_labels = balance_training(all_samples, all_labels, tot_label_list) #### without upsampling\n",
    "print(len(balanced_samples), 18 * min(get_tot_label_list()), 18 * max(get_tot_label_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_upper_balance(test_labels, maximum, new_labels):\n",
    "    #print(test_labels, new_labels)\n",
    "    for i in range(len(test_labels)):\n",
    "        if (test_labels[i] + new_labels[i] > maximum):\n",
    "            if (i != 0 and i != 1):\n",
    "                return False\n",
    "        for e in range(10):\n",
    "            if (new_labels[e] > 10): return False\n",
    "        zero_count = 0\n",
    "        for e in range(10, 18):\n",
    "            if (new_labels[e] < 1): zero_count += 1\n",
    "        if (zero_count > 4): return False\n",
    "    return True\n",
    "def check_lower_balance(test_labels, minimum):\n",
    "    for i in range(len(test_labels)):\n",
    "        if (test_labels[i] < minimum):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(test_indices, test_samples, test_labels, all_persons, samplearray, new_labels, person):\n",
    "    #print(\"before adding: \" , test_labels, \"with \", new_labels)\n",
    "    for i in range(len(samplearray)):\n",
    "        if all_persons[i] == person: # if the right person is found\n",
    "            #print(\"FOUND\")\n",
    "            test_samples.append(samplearray[i])\n",
    "            test_indices.append(i)\n",
    "    #adjust labels correctly\n",
    "    for i in range(len(new_labels)):\n",
    "        test_labels[i] += new_labels[i] \n",
    "    #print(\"after adding \" , test_labels)\n",
    "    return (test_samples, test_labels, test_indices)\n",
    "\n",
    "\n",
    "def add_test(test_indices, samplearray, lablearray):\n",
    "    test_samples = []\n",
    "    test_labels = []\n",
    "    for i in range(len(samplearray)):\n",
    "        if i in test_indices:\n",
    "            test_samples.append(samplearray[i])\n",
    "            test_labels.append(lablearray[i])\n",
    "    test_samples = np.array(test_samples)\n",
    "    test_labels = np.array(test_labels)\n",
    "    return (test_samples, test_labels)\n",
    "\n",
    "def add_training(test_indices, samplearray, lablearray):\n",
    "    train_samples = []\n",
    "    train_labels = []\n",
    "    for i in range(len(samplearray)):\n",
    "        if i not in test_indices:\n",
    "            train_samples.append(samplearray[i])\n",
    "            train_labels.append(lablearray[i])\n",
    "    train_samples = np.array(train_samples)\n",
    "    train_labels = np.array(train_labels)\n",
    "    return (train_samples, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_group(samplearray, labelarray, label_array_per_person, persons, num_diff_labels):\n",
    "    sample_min = (len(samplearray) // num_diff_labels) // 2\n",
    "    sample_max = sample_min + 10\n",
    "    print(sample_min, sample_max)\n",
    "    test_samples = []\n",
    "    test_labels = []    \n",
    "    train_samples = []\n",
    "    train_labels = []\n",
    "    test_indices = []\n",
    "    for i in range(18):\n",
    "        test_labels.append(0)\n",
    "    used_persons = []\n",
    "    balanced = False\n",
    "    index = 0\n",
    "    while (not balanced):\n",
    "        for person in range(len(persons)):\n",
    "            if (check_upper_balance(test_labels, sample_max, label_array_per_person[person])): #add the labels\n",
    "                used_persons.append(persons[person]) #add person to used person list\n",
    "                test_samples, test_labels, test_indices = add_labels(test_indices, test_samples, test_labels, all_persons, all_samples, label_array_per_person[person], str(persons[person]))\n",
    "        \n",
    "        if (check_lower_balance(test_labels, sample_min)): \n",
    "            balanced = True\n",
    "            break\n",
    "        else:\n",
    "            break\n",
    "    #print(test_labels)\n",
    "    #print(used_persons)\n",
    "    train_samples, train_labels = add_training(test_indices, samplearray, labelarray)\n",
    "    test_samples, test_labels = add_test(test_indices, samplearray, labelarray)\n",
    "    return (test_samples, test_labels, train_samples, train_labels)\n",
    "\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "train_samples = []\n",
    "train_labels = []\n",
    "persons, label_list_all_persons = prep_test_split()\n",
    "test_samples, test_labels, train_samples, train_labels = test_group(all_samples, all_labels, label_list_all_persons, persons, 18)\n",
    "print(test_labels, test_samples, len(test_samples), len(test_labels), len(train_samples), len(train_labels), len(test_samples) + len(train_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def make_testgroup(samplearray, labelarray, all_persons):\n",
    "    persons = []\n",
    "    for i in all_persons:\n",
    "        if i not in persons:\n",
    "            persons.append(i)\n",
    "    num_persons = []\n",
    "    for i in persons:\n",
    "        num_persons.append(np.count_nonzero(all_persons == i))\n",
    "        \n",
    "    label_list = []\n",
    "    for sample in all_personlabels:\n",
    "        if sample[1] == 'i003':\n",
    "            label_list.append(sample[0])\n",
    "        #label_list.append(np.count_nonzero(all_personlabels[1] == sample[0]))\n",
    "    print(label_list)\n",
    "    tot_label_list = []\n",
    "    for i in range(19):\n",
    "        tot_label_list.append(0)\n",
    "    #for label in label_list:\n",
    "     #   tot_label_list[int(label)] += 1\n",
    "    #print(tot_label_list)\n",
    "\n",
    "    \n",
    "    test_array = []\n",
    "    test_label_array = []\n",
    "    test_amount = len(samplearray) // 4.25\n",
    "    print(test_amount)\n",
    "    \n",
    "    balanced = False\n",
    "    cur_person = 0\n",
    "    while ((len(tot_label_list) < test_amount)):\n",
    "        label_list = []\n",
    "        for sample in all_personlabels:\n",
    "            if sample[1] == persons[cur_person]:\n",
    "                label_list.append(sample[0])\n",
    "        #print(label_list)\n",
    "        for label in label_list:\n",
    "            tot_label_list[int(label)] += 1\n",
    "        print(tot_label_list)\n",
    "        cur_person+=1\n",
    "    return 0\n",
    "\n",
    "make_testgroup(all_samples, all_labels, all_persons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mirror_bugged(sample):\n",
    "    mir = np.arange(sample.shape[0]*137*3).reshape(sample.shape[0], 137, 3) #create an empty copy\n",
    "    \n",
    "    for i in range(sample.shape[0]):\n",
    "        for e in range(137):\n",
    "            mir[i][e][0] = float(455) - sample[i][e][0] #mirror the x-coordinates cause input images consist of 455 by 256 pixels\n",
    "            mir[i][e][1] = sample[i][e][1]\n",
    "            mir[i][e][2] = sample[i][e][2]\n",
    "            \n",
    "    return mir #return the mirrored array\n",
    "\n",
    "\n",
    "lijst = [5, 4, 65, 85, 452, 486, 498, 2444]\n",
    "for number in lijst:\n",
    "    test = mirror(all_samples[number])\n",
    "    print(all_samples[number][2][132][0], test[2][132][0], all_samples[number][2][132][0] + test[2][132][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_minmax_left(min_lx, max_lx, min_ly, max_ly, value_x, value_y):\n",
    "        if (value_x < min_lx): min_lx = value_x\n",
    "        if (value_x > max_lx): max_lx = value_x\n",
    "        if (value_y < min_ly): min_ly = value_y\n",
    "        if (value_y > max_ly): max_ly = value_y\n",
    "            \n",
    "        return (min_lx, max_lx, min_ly, max_ly)\n",
    "    \n",
    "def update_minmax_right(min_rx, max_rx, min_ry, max_ry, value_x, value_y):\n",
    "        if (value_x < min_rx): min_rx = value_x\n",
    "        if (value_x > max_rx): max_rx = value_x\n",
    "        if (value_y < min_ry): min_ry = value_y\n",
    "        if (value_y > max_ry): max_ry = value_y\n",
    "            \n",
    "        return (min_rx, max_rx, min_ry, max_ry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that gets the features in an ordered way\n",
    "#ret[0] contains features of the 1st quarter of the frames\n",
    "#ret[1] of the 2nd quarter and so on\n",
    "#inside ret[0] (1st quarter) are the left hand x - left hand y - right hand x - right hand y values\n",
    "def avg_reach_features(samplearray):\n",
    "    values = avg_reach_extraction(samplearray)\n",
    "    values = order_features(values)\n",
    "    return values\n",
    "\n",
    "def avg_reach_extraction(samplearray):\n",
    "    ret_list = []\n",
    "    for e in range(1, 5):\n",
    "        ret_list.append(avg_reach_sample(samplearray, e))\n",
    "        #print(avg_reach_sample(samplearray, e))\n",
    "    return ret_list #contains all the features for all the quarters\n",
    "\n",
    "def avg_reach_sample(samplearray, quarter):\n",
    "    ret_list = []\n",
    "    for sample in samplearray:\n",
    "        q0 = 1\n",
    "        q1 = sample.shape[0] // 4\n",
    "        q2 = sample.shape[0] // 2\n",
    "        q3 = sample.shape[0] // 4 * 3\n",
    "        q4 = sample.shape[0]\n",
    "        lijst = [q0, q1, q2, q3, q4]\n",
    "        ret_list.append(avg_reach_quarter(sample, lijst[quarter - 1], lijst[quarter]))\n",
    "    return ret_list #contains lx, ly, rx, ry for each sample for quarter 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_reach_quarter(sample, q1, q2):\n",
    "    #input images of 455 by 256 pixels\n",
    "    min_left_y = 256\n",
    "    min_left_x = 455\n",
    "    min_right_y = 256\n",
    "    min_right_x = 455\n",
    "    max_left_x = 0\n",
    "    max_left_y = 0\n",
    "    max_right_x = 0\n",
    "    max_right_y = 0\n",
    "    \n",
    "    for i in range(q1, q2): #iterate over all the frames within the sample\n",
    "        \n",
    "        ###Calculate the average of the keypoints of the hands\n",
    "        avg_left_hand_x = sum([sample[i][e][0] for e in range(95, 116)])/21\n",
    "        avg_right_hand_x = sum([sample[i][e][0] for e in range(116, 137)])/21\n",
    "        avg_left_hand_y = sum([sample[i][e][1] for e in range(95, 116)])/21\n",
    "        avg_right_hand_y = sum([sample[i][e][1] for e in range(116, 137)])/21\n",
    "        \n",
    "        ###update the minima and maxima\n",
    "        (min_left_x, max_left_x, min_left_y, max_left_y) = update_minmax_left(min_left_x, max_left_x, min_left_y, max_left_y, avg_left_hand_x, avg_left_hand_y)\n",
    "        (min_right_x, max_right_x, min_right_y, max_right_y) = update_minmax_right(min_right_x, max_right_x, min_right_y, max_right_y, avg_right_hand_x, avg_right_hand_y)\n",
    "  \n",
    "    return (max_left_x - min_left_x, max_left_y - min_left_y, max_right_x - min_right_x, max_right_y - min_right_y)  \n",
    "\n",
    "a = avg_reach_features(all_samples)\n",
    "#keypoint1013_values = keypoint_distance_features(all_samples, 10,13)\n",
    "print(a)\n",
    "print(len(a), len(a[0]), len(a[0][0]))\n",
    "avg_reach_values = avg_reach_features(all_samples)\n",
    "#print(avg_reach_values[0][0])\n",
    "#print(avg_reach_values[0][1])\n",
    "#print(\"this works\")\n",
    "#print(total_reach_left_horizontal(all_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_reach_left_horizontal(samplearray):\n",
    "    ret_list = []\n",
    "    for sample in samplearray:\n",
    "        ans = avg_reach(sample)\n",
    "        ret_list.append(ans[1] - ans[0])\n",
    "    return ret_list\n",
    "def avg_reach_right_horizontal(samplearray):\n",
    "    ret_list = []\n",
    "    for sample in samplearray:\n",
    "        ans = avg_reach(sample)\n",
    "        ret_list.append(ans[5] - ans[4])\n",
    "    return ret_list\n",
    "def avg_reach_left_vertical(samplearray):\n",
    "    ret_list = []\n",
    "    for sample in samplearray:\n",
    "        ans = avg_reach(sample)\n",
    "        ret_list.append(ans[3] - ans[2])\n",
    "    return ret_list\n",
    "def avg_reach_right_vertical(samplearray):\n",
    "    ret_list = []\n",
    "    for sample in samplearray:\n",
    "        ans = avg_reach(sample)\n",
    "        ret_list.append(ans[7] - ans[6])\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_reach(sample):\n",
    "    #input images of 455 by 256 pixels\n",
    "    min_left_y = 256\n",
    "    min_left_x = 455\n",
    "    min_right_y = 256\n",
    "    min_right_x = 455\n",
    "    max_left_x = 0\n",
    "    max_left_y = 0\n",
    "    max_right_x = 0\n",
    "    max_right_y = 0\n",
    "    \n",
    "    for i in range(sample.shape[0]): #iterate over all the frames within the sample\n",
    "        \n",
    "        ###Calculate the average of the keypoints of the hands\n",
    "        avg_left_hand_x = sum([sample[i][e][0] for e in range(95, 116)])/21\n",
    "        avg_right_hand_x = sum([sample[i][e][0] for e in range(116, 137)])/21\n",
    "        avg_left_hand_y = sum([sample[i][e][1] for e in range(95, 116)])/21\n",
    "        avg_right_hand_y = sum([sample[i][e][1] for e in range(116, 137)])/21\n",
    "        \n",
    "        ###update the minima and maxima\n",
    "        (min_left_x, max_left_x, min_left_y, max_left_y) = update_minmax_left(min_left_x, max_left_x, min_left_y, max_left_y, avg_left_hand_x, avg_left_hand_y)\n",
    "        (min_right_x, max_right_x, min_right_y, max_right_y) = update_minmax_right(min_right_x, max_right_x, min_right_y, max_right_y, avg_right_hand_x, avg_right_hand_y)\n",
    "  \n",
    "    return (min_left_x, max_left_x, min_left_y, max_left_y, min_right_x, max_right_x, min_right_y, max_right_y)\n",
    "\n",
    "a = avg_reach(all_samples[0])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_reach_right_vertical(all_samples[0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hands_used(reach):\n",
    "    #calculate the x- and y distances\n",
    "    left_x_dis = (reach[1] - reach[0]) > 20\n",
    "    left_y_dis = (reach[3] - reach[2]) > 20\n",
    "    right_x_dis = (reach[5] - reach[4]) > 20\n",
    "    right_y_dis = (reach[7] - reach[6]) > 20\n",
    "    \n",
    "    return (\"left hand x:\", left_x_dis, \"left hand y:\" , left_y_dis, \"right hand x:\", right_x_dis, \"right hand y:\",right_y_dis)\n",
    "    \n",
    "used = hands_used(avg_reach(all_samples[0]))\n",
    "print(used)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_reach_left_horizontal(samplearray):\n",
    "    ret_list = []\n",
    "    for sample in samplearray:\n",
    "        ans = total_reach(sample)\n",
    "        ret_list.append(ans[0])\n",
    "    return ret_list\n",
    "def total_reach_right_horizontal(samplearray):\n",
    "    ret_list = []\n",
    "    for sample in samplearray:\n",
    "        ans = total_reach(sample)\n",
    "        ret_list.append(ans[2])\n",
    "    return ret_list\n",
    "def total_reach_left_vertical(samplearray):\n",
    "    ret_list = []\n",
    "    for sample in samplearray:\n",
    "        ans = total_reach(sample)\n",
    "        ret_list.append(ans[1])\n",
    "    return ret_list\n",
    "def total_reach_right_vertical(samplearray):\n",
    "    ret_list = []\n",
    "    for sample in samplearray:\n",
    "        ans = total_reach(sample)\n",
    "        ret_list.append(ans[3])\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_reach(sample):\n",
    "    coo_list = []\n",
    "    dif_list = []\n",
    "    #input images of 455 by 256 pixels\n",
    "    min_left_y = 256\n",
    "    min_left_x = 455\n",
    "    min_right_y = 256\n",
    "    min_right_x = 455\n",
    "    max_left_x = 0\n",
    "    max_left_y = 0\n",
    "    max_right_x = 0\n",
    "    max_right_y = 0\n",
    "    for i in range(sample.shape[0]):\n",
    "        #calculate the min and maxima of the fingers\n",
    "        left_point_l = min([sample[i][e][0] for e in range(95, 116)])\n",
    "        right_point_l = max([sample[i][e][0] for e in range(95, 116)])\n",
    "        bottom_point_l = min([sample[i][e][1] for e in range(95, 116)])\n",
    "        highest_point_l = max([sample[i][e][1] for e in range(95, 116)])\n",
    "        \n",
    "        left_point_r = min([sample[i][e][0] for e in range(116, 137)])\n",
    "        right_point_r = max([sample[i][e][0] for e in range(116, 137)])\n",
    "        bottom_point_r = min([sample[i][e][1] for e in range(116, 137)])\n",
    "        highest_point_r = max([sample[i][e][1] for e in range(116, 137)])\n",
    "        \n",
    "        (min_left_x, max_left_x, min_left_y, max_left_y) = update_minmax_left(min_left_x, max_left_x, min_left_y, max_left_y, left_point_l, bottom_point_l)\n",
    "        (min_left_x, max_left_x, min_left_y, max_left_y) = update_minmax_left(min_left_x, max_left_x, min_left_y, max_left_y, right_point_l, highest_point_l)\n",
    "        (min_right_x, max_right_x, min_right_y, max_right_y) = update_minmax_right(min_right_x, max_right_x, min_right_y, max_right_y, left_point_r, bottom_point_r)   \n",
    "        (min_right_x, max_right_x, min_right_y, max_right_y) = update_minmax_right(min_right_x, max_right_x, min_right_y, max_right_y, right_point_r, highest_point_r)\n",
    "        \n",
    "        #add these to the arrays\n",
    "        points = (min_left_x, max_left_x, min_left_y, max_left_y, min_right_x, max_right_x, min_right_y, max_right_y)\n",
    "        coo_list.append(points)\n",
    "        \n",
    "        differences = (points[1] - points[0], points[3] - points[2], points[5] - points[4], points[7] - points[6])\n",
    "        dif_list.append(differences) \n",
    "    \n",
    "    return differences\n",
    "        \n",
    "reaches = total_reach(all_samples[0])\n",
    "#for i in range(len(reaches[0])):\n",
    "    #print(hands_used(reaches[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(total_reach_left_vertical(all_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(samplearray):\n",
    "    ret_list = [sample.shape[0] for sample in samplearray]\n",
    "    return ret_list\n",
    "\n",
    "print(get_frames(all_samples[0:56]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_frames(samplearray): #makes sure that every sample has at least 4 frames\n",
    "    ret = []\n",
    "    for sample in samplearray:\n",
    "        if (sample.shape[0] == 1):\n",
    "            sample = np.concatenate((sample, sample), axis=0)\n",
    "            sample = np.concatenate((sample, sample), axis=0)\n",
    "        elif (sample.shape[0] == 2):\n",
    "            p0 = np.expand_dims(sample[0], axis=0)\n",
    "            p1 = np.expand_dims(sample[1], axis=0)\n",
    "            part1 = np.concatenate((p0, sample), axis=0)\n",
    "            part2 = np.concatenate((part1, p1), axis=0)\n",
    "            sample = part2\n",
    "        elif (sample.shape[0] == 3):\n",
    "            p2 = np.expand_dims(sample[2], axis = 0)\n",
    "            sample = np.concatenate((sample, p2), axis=0)\n",
    "        ret.append(sample)\n",
    "    ret = np.array(ret)\n",
    "    return ret # return the samples\n",
    "\n",
    "test = normalise_frames(all_samples)\n",
    "for sample in test:\n",
    "    if (sample.shape[0] < 4 or sample.shape[0] > 121):\n",
    "        print(sample.shape[0])\n",
    "print(get_frames(test[0:56]))\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoint_distance(samplearray, key_l, key_r):\n",
    "    ret_list = []\n",
    "    for e in range(1, 5):\n",
    "        ret_list.append(keypoint_distance_sample(samplearray, key_l, key_r, e))\n",
    "    return ret_list #contains all the features for all the quarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoint_distance_sample(samplearray, key_l, key_r, quarter):\n",
    "    ret_list = []\n",
    "    for sample in samplearray:\n",
    "        q0 = 1\n",
    "        q1 = sample.shape[0] // 4\n",
    "        q2 = sample.shape[0] // 2\n",
    "        q3 = sample.shape[0] // 4 * 3\n",
    "        q4 = sample.shape[0]\n",
    "        lijst = [q0, q1, q2, q3, q4]\n",
    "        ret_list.append(keypoint_dist2(sample, key_l, key_r, lijst[quarter - 1], lijst[quarter])[0])\n",
    "    return ret_list #contains lx, ly, rx, ry for each sample for quarter 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoint_dist2(sample, key_l, key_r, q0, q1):\n",
    "    #print(q0, q1, sample.shape[0])\n",
    "    ret_list = []   \n",
    "    dist_l_x = 0\n",
    "    dist_l_y = 0\n",
    "    dist_r_x = 0\n",
    "    dist_r_y = 0\n",
    "    #ret_list.append(keypoint_dist(sample, key_l, key_r, q0, q1, q2, q3, q4)[0])\n",
    "    for i in range(q0, q1):\n",
    "        #print(i)\n",
    "        if (sample[i][key_l][2] > 0.2 and sample[i-1][key_l][2] > 0.2):#check confidence levels\n",
    "            dist_l_x += abs(sample[i-1][key_l][0] - sample[i][key_l][0]) \n",
    "            dist_l_y += abs(sample[i-1][key_l][1] - sample[i][key_l][1]) \n",
    "        if (sample[i][key_r][2] > 0.2 and sample[i-1][key_r][2] > 0.2):#same\n",
    "            dist_r_x += abs(sample[i-1][key_r][0] - sample[i][key_r][0]) \n",
    "            dist_r_y += abs(sample[i-1][key_r][1] - sample[i][key_r][1]) \n",
    "    div = q1 - q0\n",
    "    if ((q1 - q0) == 0): div = 1\n",
    "    distances = (dist_l_x / div, dist_l_y / div, dist_r_x / div, dist_r_y / div)\n",
    "    ret_list.append(distances)\n",
    "    #print(ret_list)\n",
    "    return ret_list\n",
    "\n",
    "#Function that gets the features in an ordered way\n",
    "#ret[0] contains features of the 1st quarter of the frames\n",
    "#ret[1] of the 2nd quarter and so on\n",
    "#inside ret[0] (1st quarter) are the left hand x - left hand y - right hand x - right hand y values\n",
    "def keypoint_distance_features(samplearray, key1, key2):\n",
    "    values = keypoint_distance(samplearray, key1, key2)\n",
    "    values = order_features(values)\n",
    "    return values\n",
    "\n",
    "#HELP FUNCTION\n",
    "def order_features(values):\n",
    "    feat_array = []\n",
    "    #print(len(values[0][0]), len(values), len(values[0]))\n",
    "    for i in range(len(values)): #for every quarter\n",
    "        temp2 = []\n",
    "        for u in range(len(values[0][0])): #for every feature in the quarter\n",
    "            temp = []\n",
    "            for e in range(len(values[0])): #extract the right feature data \n",
    "                temp.append(values[i][e][u])   \n",
    "            temp2.append(temp)\n",
    "        feat_array.append(temp2)\n",
    "    #print(feat_array)\n",
    "    return feat_array\n",
    "\n",
    "#print(keypoint_distance_features(all_samples, 4, 7), len(keypoint_distance_features(all_samples, 4, 7)))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "gss = GroupShuffleSplit(n_splits=4, test_size=0.2, random_state=0)\n",
    "\n",
    "for train, test in gss.split(all_samples, all_labels, groups=all_persons):\n",
    "    print(\"%s %s\" % (train, test))\n",
    "    \n",
    "    #print(len(train), len(test))\n",
    "    label_list = [0 for e in range(18)]\n",
    "    for i in test:\n",
    "        label = all_labels[i]\n",
    "        label_list[int(label)] += 1\n",
    "    #print(label_list)\n",
    "    #execute the pipeline and get a score\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(all_samples, all_labels, test_size=0.4, random_state=0)\n",
    "\n",
    "print(len(X_train), len(y_train))\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "#cross_val_score(clf, all_samples, all_labels, cv=cv)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for split in lijst:\n",
    "    #pipeline\n",
    "    #optimale C waarde\n",
    "    #score hou je bij\n",
    "gem van alle C-waarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2) # project to 2 dimensions\n",
    "\n",
    "\n",
    "print(np.shape(all_samples))\n",
    "print(all_samples)\n",
    "projected = pca.fit_transform()\n",
    "\n",
    "\n",
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=all_labels, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('spectral', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();\n",
    "print(projected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help functions for the gifs to fix the gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# REQUIRED IMPORTS FROM STANDARD PACKAGES\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "from os.path import join as pjoin\n",
    "from glob import glob\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# IMPORTS FROM THE UTIL LIBRARY PROVIDED BY US\n",
    "\n",
    "import util.vis as V\n",
    "import util.vis4 as V_gif\n",
    "import util.helpers as H\n",
    "\n",
    "# Normally, all libraries are loaded only once, \n",
    "# even if you execute the import code multiple times\n",
    "# This code is helpful if you make your own helper libraries \n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "# list your libraries below with aimport: should ensure \n",
    "#they are reloaded each time without having to restart your kernel\n",
    "# in this case, our libraries are used as an example\n",
    "\n",
    "%aimport util.helpers, util.vis\n",
    "%aimport features_extraction\n",
    "%aimport augmentation\n",
    "%aimport Analysis\n",
    "%aimport validation\n",
    "%aimport preprocessing\n",
    "%aimport upsampling\n",
    "\n",
    "# seed random generator such that this notebook always returns the same values \n",
    "# (this is by no means necessary, but it is useful for reproducability of results)\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "\n",
    "DATA_DIR = '../data'\n",
    "POSE_DIR = '../data/pose'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ##Loading all the training data\n",
    "dataset_file=pjoin(DATA_DIR,'labels.csv')\n",
    "\n",
    "train_samples=[]\n",
    "train_labels=[]\n",
    "train_persons = []\n",
    "train_personlabels = []\n",
    "\n",
    "with open(dataset_file) as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    sample_index = 0\n",
    "    outliers = []\n",
    "    #outliers = [229, 297, 1316, 1780, 1817, 49, 495, 541, 746, 878, 2910]\n",
    "    #add [2538, 1753, 1327, 2954, 2956, 2958, 4, 1111, 1117, 1118, 2167, 1992, 1993, 1573, 1575, 1576, 977, 245] clean\n",
    "    #not clean certain = [1795, 1836, 1899, 2425, 2535, 2580, 2805, 3504]\n",
    "    #not clean outliers_robbe_folds = [393, 418, 431, 491, 671, 713, 789, 809, 815, 876, 970, 984, 1009, 1028, 1104, 1288, 1301, 1516, 1556, 1578, 1625, 1626, 1795]\n",
    "    outliers_folds_robbe = [2538, 1753, 1327, 2954, 2956, 2958, 4, 1111, 1117, 1118, 2167, 1992, 1993, 1573, 1575, 1576, 977, 245, 1846, 1909, 2435, 2545, 2590, 2815, 3515, 396, 421, 434, 494, 676, 718, 795, 815, 821, 882, 977, 991, 1016, 1035, 1111, 1295, 1308, 1524, 1564, 1586, 1633, 1634, 1804]\n",
    "    for row in reader: \n",
    "        name, _gloss, label, _person = row\n",
    "        sample = np.load(pjoin(POSE_DIR, 'train', name+'.npy'))\n",
    "        if sample_index not in outliers:\n",
    "            train_samples.append(sample)\n",
    "            train_labels.append(int(label))\n",
    "            train_persons.append(_person)\n",
    "            train_personlabels.append((label, _person))\n",
    "        sample_index += 1\n",
    "\n",
    "train_samples=np.array(train_samples)\n",
    "train_labels=np.array(train_labels)\n",
    "\n",
    "##Loading all the test data\n",
    "all_test_files = sorted(glob(pjoin(POSE_DIR, 'test', '*.npy')))  \n",
    "\n",
    "test_samples = []\n",
    "for numpy_file in all_test_files:\n",
    "    sample = np.load(numpy_file)\n",
    "    test_samples.append(sample)\n",
    "    \n",
    "test_samples = np.array(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2538, 1753, 1327, 2954, 2956, 2958, 4, 1111, 1117, 1118, 2167, 1992, 1993, 1573, 1575, 1576, 977, 245, 1846, 1909, 2435, 2545, 2590, 2815, 3515, 396, 421, 434, 494, 676, 718, 795, 815, 821, 882, 977, 991, 1016, 1035, 1111, 1295, 1308, 1524, 1564, 1586, 1633, 1634, 1804]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "clean_robbe = [2538, 1753, 1327, 2954, 2956, 2958, 4, 1111, 1117, 1118, 2167, 1992, 1993, 1573, 1575, 1576, 977, 245]\n",
    "not_clean_robbe = [1836, 1899, 2425, 2535, 2580, 2805, 3504, 393, 418, 431, 491, 671, 713, 789, 809, 815, 876, 970, 984, 1009, 1028, 1104, 1288, 1301, 1516, 1556, 1578, 1625, 1626, 1795]\n",
    "for i in range(len(not_clean_robbe)):\n",
    "    if not_clean_robbe[i] > 2910:\n",
    "        clean_robbe.append(not_clean_robbe[i] + 11)\n",
    "    elif not_clean_robbe[i] > 1817:\n",
    "        clean_robbe.append(not_clean_robbe[i] + 10)\n",
    "    elif not_clean_robbe[i] > 1780:\n",
    "        clean_robbe.append(not_clean_robbe[i] + 9)\n",
    "    elif not_clean_robbe[i] > 1316:\n",
    "        clean_robbe.append(not_clean_robbe[i] + 8)\n",
    "    elif not_clean_robbe[i] > 878:\n",
    "        clean_robbe.append(not_clean_robbe[i] + 7)\n",
    "    elif not_clean_robbe[i] > 746:\n",
    "        clean_robbe.append(not_clean_robbe[i] + 6)\n",
    "    elif not_clean_robbe[i] > 541:\n",
    "        clean_robbe.append(not_clean_robbe[i] + 5)\n",
    "    elif not_clean_robbe[i] > 495:\n",
    "        clean_robbe.append(not_clean_robbe[i] + 4)\n",
    "    elif not_clean_robbe[i] > 297:\n",
    "        clean_robbe.append(not_clean_robbe[i] + 3)\n",
    "    elif not_clean_robbe[i] > 229:\n",
    "        clean_robbe.append(not_clean_robbe[i] + 2)\n",
    "    elif not_clean_robbe[i] > 49:\n",
    "        clean_robbe.append(not_clean_robbe[i] + 1)\n",
    "print(clean_robbe)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_samples = preprocessing.rotate(train_samples)\n",
    "#train_samples = preprocessing.scale(train_samples)\n",
    "#train_samples = preprocessing.centering(train_samples)\n",
    "\n",
    "#test_samples = preprocessing.rotate(test_samples)\n",
    "#test_samples = preprocessing.scale(test_samples)\n",
    "#test_samples = preprocessing.centering(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 19, 20, 21, 22, 24, 25, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 49, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 68, 69, 70, 72, 73, 76, 78, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 114, 115, 116, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 143, 144, 145, 146, 147, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 167, 170, 171, 172, 173, 177, 178, 179, 181, 184, 185, 186, 187, 189, 190, 191, 193, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 233, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 262, 263, 264, 265, 266, 267, 269, 270, 271, 272, 273, 274, 275, 276, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 291, 292, 293, 295, 297, 299, 300, 301, 302, 304, 305, 306, 307, 308, 310, 311, 313, 314, 317, 318, 319, 320, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 334, 335, 336, 337, 339, 340, 341, 342, 344, 345, 346, 347, 348, 349, 351, 352, 355, 356, 357, 358, 359, 362, 363, 364, 365, 367, 369, 370, 371, 372, 373, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 388, 389, 390, 391, 392, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 409, 411, 412, 413, 414, 415, 416, 417, 419, 420, 421, 422, 423, 424, 427, 432, 433, 434, 435, 436, 439, 440, 441, 443, 444, 445, 446, 447, 448, 449, 450, 451, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 466, 467, 468, 469, 470, 471, 472, 473, 474, 476, 478, 479, 481, 482, 484, 486, 487, 488, 489, 490, 492, 493, 494, 495, 496, 497, 499, 500, 501, 502, 504, 505, 506, 507, 509, 510, 511, 512, 513, 514, 515, 516, 517, 519, 520, 522, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536, 537, 538, 540, 541, 542, 543, 544, 545, 549, 550, 551, 552, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 567, 568, 570, 572, 573, 575, 577, 579, 581, 582, 584, 585, 586, 587, 589, 590, 593, 594, 595, 596, 598, 599, 600, 601, 603, 604, 605, 606, 607, 609, 610, 611, 614, 615, 616, 621, 622, 623, 624, 625, 626, 627, 628, 630, 631, 632, 633, 634, 636, 637, 638, 639, 641, 642, 643, 645, 647, 650, 651, 653, 654, 655, 657, 658, 660, 661, 662, 664, 665, 666, 667, 668, 669, 670, 672, 673, 674, 675, 676, 678, 679, 680, 681, 682, 684, 685, 686, 688, 689, 690, 692, 693, 694, 695, 696, 697, 698, 700, 701, 702, 703, 704, 705, 707, 708, 709, 710, 711, 712, 714, 715, 718, 719, 722, 723, 724, 726, 727, 728, 729, 730, 731, 732, 736, 737, 739, 740, 741, 743, 744, 746, 747, 748, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 771, 772, 773, 774, 775, 776, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 790, 791, 792, 793, 795, 796, 798, 799, 800, 801, 802, 803, 804, 805, 806, 808, 810, 812, 814, 816, 817, 819, 821, 822, 823, 825, 826, 827, 829, 830, 831, 832, 833, 834, 835, 837, 838, 839, 840, 841, 842, 843, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 857, 858, 859, 860, 861, 862, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 877, 878, 879, 880, 881, 882, 883, 884, 886, 887, 888, 889, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 907, 908, 909, 910, 911, 912, 913, 916, 917, 918, 919, 920, 921, 925, 926, 927, 928, 929, 931, 932, 934, 935, 936, 937, 938, 939, 940, 941, 942, 944, 945, 946, 947, 948, 949, 950, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 971, 972, 975, 976, 977, 978, 979, 980, 981, 982, 983, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 997, 998, 1000, 1001, 1003, 1004, 1005, 1007, 1008, 1011, 1013, 1014, 1015, 1016, 1017, 1018, 1020, 1021, 1023, 1024, 1025, 1026, 1027, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1051, 1052, 1053, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1064, 1066, 1068, 1069, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1081, 1083, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1093, 1095, 1096, 1098, 1099, 1101, 1102, 1103, 1105, 1106, 1107, 1108, 1110, 1111, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1141, 1142, 1143, 1144, 1145, 1149, 1150, 1151, 1152, 1154, 1156, 1157, 1158, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1170, 1171, 1172, 1173, 1174, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1227, 1228, 1229, 1230, 1232, 1235, 1236, 1237, 1238, 1239, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1257, 1258, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1277, 1279, 1281, 1282, 1284, 1286, 1287, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1302, 1303, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1321, 1322, 1323, 1324, 1325, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1345, 1346, 1347, 1348, 1349, 1350, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1364, 1365, 1367, 1368, 1371, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1387, 1388, 1389, 1390, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1423, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1439, 1440, 1442, 1443, 1444, 1445, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1462, 1463, 1465, 1466, 1467, 1468, 1469, 1470, 1472, 1473, 1474, 1475, 1476, 1479, 1481, 1482, 1483, 1485, 1486, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1507, 1508, 1509, 1511, 1512, 1513, 1514, 1515, 1517, 1518, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1542, 1544, 1545, 1546, 1547, 1548, 1550, 1551, 1552, 1553, 1554, 1555, 1557, 1558, 1559, 1560, 1561, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1572, 1573, 1575, 1576, 1577, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1588, 1589, 1590, 1591, 1592, 1594, 1595, 1599, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1617, 1619, 1620, 1621, 1622, 1624, 1627, 1628, 1629, 1630, 1632, 1633, 1635, 1636, 1637, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1654, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1693, 1694, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1704, 1705, 1706, 1707, 1708, 1709, 1711, 1712, 1713, 1714, 1715, 1716, 1719, 1720, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1738, 1739, 1740, 1741, 1742, 1744, 1745, 1746, 1747, 1749, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1769, 1770, 1771, 1772, 1773, 1775, 1776, 1777, 1778, 1781, 1782, 1784, 1785, 1786, 1787, 1788, 1791, 1792, 1793, 1797, 1800, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1811, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1837, 1838, 1840, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1860, 1861, 1862, 1863, 1864, 1865, 1867, 1868, 1869, 1870, 1872, 1873, 1874, 1875, 1877, 1878, 1879, 1880, 1881, 1882, 1885, 1886, 1887, 1888, 1889, 1890, 1892, 1893, 1895, 1896, 1897, 1898, 1900, 1902, 1904, 1905, 1906, 1908, 1909, 1910, 1911, 1912, 1915, 1916, 1917, 1918, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1933, 1934, 1936, 1939, 1940, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1950, 1951, 1952, 1953, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1969, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1996, 1997, 1998, 2000, 2001, 2004, 2006, 2008, 2009, 2011, 2013, 2014, 2015, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2034, 2035, 2036, 2037, 2039, 2040, 2041, 2042, 2043, 2045, 2046, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2068, 2070, 2071, 2072, 2073, 2074, 2075, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2087, 2088, 2089, 2090, 2091, 2092, 2094, 2095, 2096, 2097, 2098, 2100, 2101, 2102, 2103, 2105, 2106, 2107, 2108, 2109, 2110, 2114, 2117, 2118, 2119, 2123, 2124, 2125, 2127, 2129, 2130, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2150, 2151, 2154, 2156, 2157, 2158, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2193, 2194, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2205, 2206, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2219, 2220, 2221, 2222, 2223, 2225, 2226, 2227, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2248, 2249, 2250, 2251, 2252, 2255, 2256, 2257, 2258, 2259, 2262, 2264, 2265, 2266, 2267, 2268, 2270, 2271, 2272, 2274, 2275, 2276, 2278, 2279, 2280, 2281, 2282, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2293, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2308, 2309, 2310, 2311, 2312, 2316, 2317, 2318, 2319, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2336, 2337, 2338, 2340, 2341, 2342, 2343, 2345, 2346, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2358, 2359, 2360, 2361, 2362, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2377, 2378, 2379, 2380, 2381, 2383, 2385, 2386, 2387, 2388, 2390, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2400, 2401, 2402, 2403, 2404, 2406, 2407, 2408, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2418, 2419, 2420, 2422, 2424, 2426, 2427, 2429, 2430, 2431, 2432, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2445, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2455, 2457, 2458, 2459, 2460, 2461, 2464, 2465, 2466, 2467, 2469, 2470, 2471, 2472, 2473, 2474, 2476, 2478, 2479, 2480, 2481, 2483, 2484, 2485, 2486, 2489, 2490, 2491, 2492, 2494, 2495, 2497, 2499, 2501, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2512, 2513, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2528, 2529, 2530, 2532, 2534, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2548, 2549, 2551, 2553, 2554, 2555, 2556, 2557, 2559, 2562, 2563, 2564, 2565, 2567, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2577, 2578, 2582, 2583, 2584, 2585, 2586, 2588, 2589, 2592, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2608, 2609, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2629, 2630, 2632, 2633, 2637, 2638, 2639, 2641, 2642, 2643, 2644, 2645, 2646, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2658, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2670, 2671, 2672, 2673, 2674, 2675, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2705, 2706, 2708, 2709, 2710, 2712, 2716, 2717, 2718, 2719, 2723, 2724, 2725, 2726, 2727, 2728, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2741, 2742, 2743, 2745, 2746, 2747, 2748, 2749, 2751, 2752, 2753, 2755, 2756, 2757, 2758, 2759, 2760, 2762, 2763, 2764, 2765, 2766, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2777, 2778, 2779, 2781, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2792, 2793, 2794, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2806, 2807, 2808, 2809, 2810, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2842, 2843, 2844, 2846, 2847, 2848, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2866, 2867, 2868, 2869, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2888, 2889, 2890, 2892, 2894, 2895, 2896, 2897, 2898, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2920, 2921, 2922, 2924, 2925, 2926, 2927, 2928, 2929, 2932, 2934, 2935, 2936, 2937, 2938, 2939, 2941, 2942, 2943, 2944, 2945, 2946, 2949, 2950, 2952, 2953, 2956, 2957, 2958, 2959, 2960, 2962, 2963, 2964, 2965, 2966, 2967, 2969, 2973, 2974, 2977, 2978, 2979, 2981, 2982, 2983, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3005, 3006, 3007, 3009, 3011, 3012, 3014, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3025, 3026, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3037, 3038, 3039, 3040, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3076, 3078, 3080, 3081, 3082, 3083, 3084, 3085, 3087, 3088, 3091, 3092, 3093, 3095, 3096, 3097, 3098, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3113, 3114, 3115, 3117, 3119, 3120, 3121, 3122, 3123, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3137, 3138, 3139, 3140, 3141, 3142, 3144, 3145, 3147, 3148, 3149, 3150, 3152, 3153, 3154, 3155, 3157, 3158, 3159, 3160, 3161, 3163, 3164, 3166, 3167, 3168, 3169, 3170, 3171, 3173, 3174, 3175, 3176, 3179, 3183, 3185, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195, 3196, 3199, 3200, 3201, 3202, 3204, 3205, 3206, 3207, 3209, 3212, 3215, 3216, 3217, 3219, 3220, 3221, 3222, 3223, 3224, 3226, 3228, 3229, 3231, 3233, 3234, 3235, 3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247, 3248, 3249, 3251, 3252, 3253, 3254, 3255, 3257, 3258, 3259, 3260, 3261, 3262, 3265, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3284, 3287, 3288, 3289, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312, 3313, 3314, 3315, 3317, 3318, 3319, 3320, 3321, 3322, 3323, 3324, 3325, 3327, 3328, 3330, 3331, 3333, 3334, 3335, 3336, 3337, 3339, 3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3348, 3349, 3350, 3353, 3354, 3355, 3356, 3357, 3358, 3360, 3361, 3363, 3365, 3367, 3368, 3370, 3371, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3401, 3402, 3403, 3405, 3406, 3407, 3408, 3409, 3411, 3412, 3413, 3414, 3415, 3416, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3426, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3453, 3454, 3456, 3457, 3458, 3460, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3478, 3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489, 3492, 3493, 3494, 3495, 3497, 3498, 3500, 3501, 3502, 3503, 3506, 3508, 3509, 3510, 3511, 3512, 3513, 3514, 3516, 3517, 3518, 3519, 3520, 3522, 3523, 3524, 3525, 3526, 3528, 3530, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541, 3542, 3543, 3544, 3545, 3546, 3548, 3549, 3550, 3552, 3553, 3554, 3555, 3556, 3558, 3559, 3560, 3561, 3563, 3564, 3565, 3566, 3567, 3568, 3571, 3573, 3574, 3575, 3577, 3578, 3579, 3580, 3581, 3583, 3584, 3585, 3586, 3587, 3588, 3589, 3590, 3592, 3595, 3596, 3597, 3598, 3599, 3601, 3602, 3603, 3606, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3618, 3619, 3621, 3624, 3625, 3627, 3628, 3630, 3631, 3632, 3633, 3634, 3636, 3637, 3638, 3639, 3640, 3641, 3645, 3646, 3647, 3649, 3650, 3651, 3652, 3653, 3655, 3656, 3657, 3659, 3660, 3661, 3662, 3664, 3666, 3667, 3668, 3669, 3670, 3671, 3673, 3674, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3686, 3687, 3688, 3689, 3690, 3691, 3693, 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3702, 3703, 3704, 3705, 3706] 2956\n"
     ]
    }
   ],
   "source": [
    "seed = 96\n",
    "sgkf = validation.stratified_group_k_fold(train_samples, train_labels, train_persons, 5, seed)\n",
    "fold96_1 = []\n",
    "for fold_ind, (dev_ind, val_ind) in enumerate(sgkf):\n",
    "    if (fold_ind == 0):\n",
    "        fold96_1 = dev_ind\n",
    "        \n",
    "seed = 11\n",
    "sgkf = validation.stratified_group_k_fold(train_samples, train_labels, train_persons, 5, seed)\n",
    "fold11_4= []\n",
    "for fold_ind, (dev_ind, val_ind) in enumerate(sgkf):\n",
    "    if (fold_ind == 4):\n",
    "        fold11_4 = dev_ind\n",
    "        \n",
    "seed = 87\n",
    "sgkf = validation.stratified_group_k_fold(train_samples, train_labels, train_persons, 5, seed)\n",
    "fold87_1= []\n",
    "for fold_ind, (dev_ind, val_ind) in enumerate(sgkf):\n",
    "    if (fold_ind == 1):\n",
    "        fold87_1 = dev_ind\n",
    "        print(fold87_1, len(fold87_1))\n",
    "        fold87_1_val = val_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "============================\n",
      "============================\n",
      "============================\n",
      "============================\n",
      "[0, 13, 17, 23, 27, 29, 38, 39, 48, 50, 52, 63, 67, 71, 74, 75, 77, 79, 81, 94, 98, 99, 111, 112, 113, 117, 118, 121, 134, 142, 148, 149, 150, 161, 166, 168, 169, 174, 175, 176, 180, 182, 183, 188, 192, 194, 208, 209, 210, 212, 232, 234, 235, 245, 259, 268, 277, 290, 294, 296, 298, 303, 309, 312, 315, 316, 321, 327, 333, 338, 343, 350, 353, 354, 360, 361, 366, 368, 374, 386, 387, 393, 408, 410, 418, 425, 426, 428, 429, 430, 431, 437, 438, 442, 452, 465, 475, 477, 480, 483, 485, 491, 498, 503, 508, 518, 521, 523, 529, 533, 539, 546, 547, 548, 553, 554, 555, 566, 569, 571, 574, 576, 578, 580, 583, 588, 591, 592, 597, 602, 608, 612, 613, 617, 618, 619, 620, 629, 635, 640, 644, 646, 648, 649, 652, 656, 659, 663, 671, 677, 683, 687, 691, 699, 706, 713, 716, 717, 720, 721, 725, 733, 734, 735, 738, 742, 745, 749, 758, 769, 770, 777, 789, 794, 797, 807, 809, 811, 813, 815, 818, 820, 824, 828, 836, 844, 855, 856, 863, 876, 885, 890, 906, 914, 915, 922, 923, 924, 930, 933, 943, 951, 970, 973, 974, 984, 995, 996, 999, 1002, 1006, 1009, 1010, 1012, 1019, 1022, 1028, 1037, 1038, 1050, 1054, 1063, 1065, 1067, 1070, 1071, 1072, 1080, 1082, 1084, 1092, 1094, 1097, 1100, 1104, 1109, 1112, 1132, 1140, 1146, 1147, 1148, 1153, 1155, 1159, 1168, 1169, 1175, 1193, 1207, 1217, 1218, 1226, 1231, 1233, 1234, 1240, 1253, 1254, 1255, 1256, 1259, 1275, 1276, 1278, 1280, 1283, 1285, 1288, 1289, 1300, 1301, 1304, 1320, 1326, 1335, 1344, 1351, 1363, 1366, 1369, 1370, 1372, 1386, 1391, 1402, 1411, 1422, 1424, 1425, 1438, 1441, 1446, 1461, 1464, 1471, 1477, 1478, 1480, 1484, 1487, 1488, 1506, 1510, 1516, 1519, 1520, 1530, 1541, 1543, 1549, 1556, 1562, 1570, 1571, 1574, 1578, 1586, 1587, 1593, 1596, 1597, 1598, 1600, 1616, 1618, 1623, 1625, 1626, 1631, 1634, 1638, 1653, 1655, 1679, 1680, 1681, 1691, 1692, 1695, 1703, 1710, 1717, 1718, 1721, 1737, 1743, 1748, 1750, 1760, 1768, 1774, 1779, 1780, 1783, 1789, 1790, 1794, 1795, 1796, 1798, 1799, 1801, 1810, 1812, 1824, 1825, 1836, 1839, 1841, 1859, 1866, 1871, 1876, 1883, 1884, 1891, 1894, 1899, 1901, 1903, 1907, 1913, 1914, 1919, 1932, 1935, 1937, 1938, 1941, 1949, 1954, 1967, 1968, 1970, 1992, 1993, 1994, 1995, 1999, 2002, 2003, 2005, 2007, 2010, 2012, 2016, 2025, 2033, 2038, 2044, 2047, 2059, 2067, 2069, 2076, 2086, 2093, 2099, 2104, 2111, 2112, 2113, 2115, 2116, 2120, 2121, 2122, 2126, 2128, 2131, 2139, 2148, 2149, 2152, 2153, 2155, 2159, 2179, 2192, 2195, 2204, 2207, 2217, 2218, 2224, 2228, 2239, 2247, 2253, 2254, 2260, 2261, 2263, 2269, 2273, 2277, 2283, 2291, 2292, 2294, 2295, 2296, 2306, 2307, 2313, 2314, 2315, 2320, 2333, 2334, 2335, 2339, 2344, 2347, 2348, 2357, 2363, 2375, 2376, 2382, 2384, 2389, 2391, 2399, 2405, 2409, 2417, 2421, 2423, 2425, 2428, 2433, 2444, 2446, 2454, 2456, 2462, 2463, 2468, 2475, 2477, 2482, 2487, 2488, 2493, 2496, 2498, 2500, 2502, 2511, 2514, 2527, 2531, 2533, 2535, 2546, 2547, 2550, 2552, 2558, 2560, 2561, 2566, 2568, 2576, 2579, 2580, 2581, 2587, 2590, 2591, 2593, 2594, 2607, 2610, 2618, 2628, 2631, 2634, 2635, 2636, 2640, 2647, 2657, 2659, 2669, 2676, 2677, 2685, 2696, 2704, 2707, 2711, 2713, 2714, 2715, 2720, 2721, 2722, 2729, 2740, 2744, 2750, 2754, 2761, 2767, 2776, 2780, 2782, 2791, 2795, 2805, 2811, 2823, 2837, 2840, 2845, 2849, 2850, 2865, 2870, 2879, 2887, 2891, 2893, 2899, 2900, 2901, 2919, 2923, 2930, 2931, 2933, 2940, 2947, 2948, 2951, 2954, 2955, 2961, 2968, 2970, 2971, 2972, 2975, 2976, 2980, 2984, 2994, 3004, 3008, 3010, 3013, 3015, 3024, 3027, 3036, 3041, 3054, 3075, 3077, 3079, 3086, 3089, 3090, 3094, 3099, 3112, 3116, 3118, 3124, 3136, 3143, 3146, 3151, 3156, 3162, 3165, 3172, 3177, 3178, 3180, 3181, 3182, 3184, 3186, 3197, 3198, 3203, 3208, 3210, 3211, 3213, 3214, 3218, 3225, 3227, 3230, 3232, 3250, 3256, 3263, 3264, 3266, 3274, 3275, 3283, 3285, 3286, 3290, 3300, 3316, 3326, 3329, 3332, 3338, 3351, 3352, 3359, 3362, 3364, 3366, 3369, 3372, 3373, 3374, 3387, 3400, 3404, 3410, 3417, 3425, 3441, 3442, 3443, 3452, 3455, 3459, 3461, 3479, 3490, 3491, 3496, 3499, 3504, 3505, 3507, 3515, 3521, 3527, 3529, 3531, 3547, 3551, 3557, 3562, 3569, 3570, 3572, 3576, 3582, 3591, 3593, 3594, 3600, 3604, 3605, 3607, 3608, 3617, 3620, 3622, 3623, 3626, 3629, 3635, 3642, 3643, 3644, 3648, 3654, 3658, 3663, 3665, 3672, 3675, 3684, 3685, 3692, 3701] 751\n"
     ]
    }
   ],
   "source": [
    "seed = 24\n",
    "sgkf = validation.stratified_group_k_fold(train_samples, train_labels, train_persons, 5, seed)\n",
    "\n",
    "fold2 = []\n",
    "fold3 = []\n",
    "\n",
    "for fold_ind, (dev_ind, val_ind) in enumerate(sgkf):\n",
    "    #print(fold_ind, dev_ind)\n",
    "    if (fold_ind == 2):\n",
    "        #print(dev_ind)\n",
    "        fold2 = dev_ind\n",
    "    if (fold_ind == 3):\n",
    "        fold3 = dev_ind\n",
    "    print(\"============================\")\n",
    "\n",
    "fold_together = [i for i in fold2 if i in fold3 and i in fold96_1 and i in fold11_4]\n",
    "print(fold_together, len(fold_together))\n",
    "\n",
    "lijst = fold_together\n",
    "#lijst = [lijst for lijst in lijst if lijst > 2100]\n",
    "#print(lijst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "seed = 126\n",
    "sgkf = validation.stratified_group_k_fold(train_samples, train_labels, train_persons, 5, seed)\n",
    "fold126_2 = []\n",
    "for fold_ind, (dev_ind, val_ind) in enumerate(sgkf):\n",
    "    if (fold_ind == 2):\n",
    "        fold126_2 = dev_ind\n",
    "        \n",
    "seed = 667\n",
    "sgkf = validation.stratified_group_k_fold(train_samples, train_labels, train_persons, 5, seed)\n",
    "fold667_2 = []\n",
    "for fold_ind, (dev_ind, val_ind) in enumerate(sgkf):\n",
    "    if (fold_ind == 2):\n",
    "        fold667_2 = dev_ind      \n",
    "        \n",
    "seed = 159\n",
    "sgkf = validation.stratified_group_k_fold(train_samples, train_labels, train_persons, 5, seed)\n",
    "fold159_2 = []\n",
    "for fold_ind, (dev_ind, val_ind) in enumerate(sgkf):\n",
    "    if (fold_ind == 2):\n",
    "        fold159_2 = dev_ind  \n",
    "        \n",
    "print(fold159_2 == fold667_2)\n",
    "\n",
    "\n",
    "        \n",
    "seed = 28\n",
    "sgkf = validation.stratified_group_k_fold(train_samples, train_labels, train_persons, 10, seed)\n",
    "fold28_3 = []\n",
    "fold28_1 = []\n",
    "fold28_5 = []\n",
    "fold28_8 = []\n",
    "fold28_9 = []\n",
    "for fold_ind, (dev_ind, val_ind) in enumerate(sgkf):\n",
    "    if (fold_ind == 1):\n",
    "        fold28_1 = dev_ind\n",
    "    if (fold_ind == 3):\n",
    "        fold28_3 = dev_ind \n",
    "    if (fold_ind == 5):\n",
    "        fold28_5 = dev_ind \n",
    "    if (fold_ind == 8):\n",
    "        fold28_8 = dev_ind \n",
    "    if (fold_ind == 9):\n",
    "        fold28_9 = dev_ind \n",
    "        \n",
    "        \n",
    "        \n",
    "seed = 251\n",
    "sgkf = validation.stratified_group_k_fold(train_samples, train_labels, train_persons, 20, seed)\n",
    "fold251_8 = []\n",
    "fold251_11 = []\n",
    "fold251_16 = []\n",
    "fold251_1 = []\n",
    "fold251_2 = []\n",
    "fold251_5 = []\n",
    "fold251_18 = []\n",
    "for fold_ind, (dev_ind, val_ind) in enumerate(sgkf):\n",
    "    if (fold_ind == 8):\n",
    "        fold251_8 = dev_ind\n",
    "    if (fold_ind == 11):\n",
    "        fold251_11 = dev_ind \n",
    "    if (fold_ind == 16):\n",
    "        fold251_16 = dev_ind \n",
    "    if (fold_ind == 1):\n",
    "        fold251_1 = dev_ind\n",
    "    if (fold_ind == 2):\n",
    "        fold251_2 = dev_ind \n",
    "    if (fold_ind == 5):\n",
    "        fold251_5 = dev_ind \n",
    "    if (fold_ind == 18):\n",
    "        fold251_18 = dev_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 4, 5, 6, 9, 10, 14, 15, 19, 23, 35, 36, 38, 39, 40, 44, 47, 50, 54, 56, 59, 60, 61, 65, 70, 73, 76, 77, 79, 81, 82, 85, 87, 91, 93, 95, 96, 98, 99, 104, 107, 110, 111, 112, 116, 117, 118, 119, 121, 122, 127, 129, 131, 132, 133, 136, 141, 147, 151, 152, 157, 163, 168, 171, 174, 176, 177, 179, 187, 190, 192, 194, 195, 199, 200, 207, 208, 210, 213, 218, 219, 220, 224, 225, 227, 231, 234, 239, 241, 242, 244, 245, 247, 251, 253, 256, 258, 260, 263, 264, 265, 268, 270, 275, 280, 281, 283, 286, 289, 293, 296, 297, 298, 299, 300, 308, 311, 313, 314, 315, 320, 322, 325, 326, 327, 328, 329, 331, 344, 355, 356, 358, 359, 360, 362, 368, 369, 377, 378, 379, 380, 386, 387, 391, 399, 400, 402, 404, 415, 420, 422, 423, 426, 435, 439, 444, 448, 449, 450, 451, 452, 453, 454, 456, 460, 461, 465, 469, 471, 472, 473, 475, 479, 484, 485, 490, 491, 493, 498, 504, 505, 508, 509, 511, 514, 515, 519, 523, 524, 526, 528, 531, 534, 535, 540, 543, 546, 551, 552, 555, 561, 563, 564, 567, 570, 571, 572, 573, 575, 578, 582, 586, 589, 590, 591, 592, 595, 598, 600, 603, 610, 612, 620, 628, 629, 630, 631, 633, 636, 637, 641, 642, 646, 648, 652, 654, 656, 659, 662, 663, 666, 667, 670, 674, 680, 681, 683, 685, 687, 689, 691, 692, 694, 697, 698, 700, 701, 704, 706, 707, 708, 712, 713, 717, 723, 726, 729, 732, 735, 737, 738, 743, 747, 748, 757, 758, 759, 761, 763, 766, 768, 769, 770, 775, 777, 779, 789, 793, 798, 799, 800, 805, 806, 807, 808, 810, 811, 812, 814, 815, 816, 817, 820, 826, 827, 831, 833, 834, 837, 839, 840, 846, 851, 852, 854, 855, 856, 857, 858, 861, 863, 865, 867, 871, 872, 876, 877, 879, 880, 885, 887, 888, 890, 894, 895, 901, 904, 906, 910, 912, 916, 917, 918, 919, 923, 928, 933, 934, 936, 938, 939, 941, 943, 944, 949, 951, 953, 954, 955, 957, 962, 963, 964, 966, 967, 968, 969, 972, 975, 979, 982, 983, 986, 989, 990, 992, 995, 996, 999, 1001, 1006, 1010, 1011, 1015, 1018, 1019, 1020, 1021, 1025, 1032, 1033, 1035, 1038, 1043, 1048, 1051, 1052, 1060, 1063, 1065, 1068, 1069, 1076, 1077, 1078, 1080, 1081, 1084, 1090, 1091, 1092, 1093, 1094, 1097, 1098, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1117, 1120, 1124, 1127, 1134, 1136, 1141, 1143, 1148, 1150, 1151, 1152, 1153, 1154, 1159, 1161, 1162, 1164, 1165, 1169, 1170, 1171, 1173, 1176, 1178, 1179, 1182, 1183, 1184, 1189, 1192, 1193, 1196, 1197, 1198, 1199, 1203, 1206, 1209, 1210, 1211, 1213, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1231, 1234, 1235, 1238, 1240, 1241, 1242, 1246, 1247, 1248, 1249, 1252, 1254, 1257, 1258, 1259, 1261, 1262, 1264, 1267, 1273, 1279, 1280, 1281, 1282, 1284, 1289, 1295, 1296, 1297, 1298, 1307, 1308, 1309, 1312, 1313, 1314, 1318, 1323, 1324, 1328, 1329, 1330, 1333, 1335, 1336, 1337, 1338, 1340, 1344, 1345, 1347, 1349, 1351, 1352, 1353, 1354, 1355, 1356, 1361, 1364, 1365, 1367, 1369, 1370, 1376, 1379, 1383, 1384, 1387, 1388, 1391, 1393, 1394, 1395, 1396, 1398, 1399, 1400, 1401, 1403, 1404, 1407, 1408, 1409, 1411, 1413, 1414, 1415, 1417, 1418, 1422, 1423, 1424, 1426, 1429, 1432, 1434, 1436, 1437, 1439, 1440, 1441, 1442, 1445, 1446, 1449, 1450, 1452, 1453, 1456, 1457, 1464, 1466, 1467, 1468, 1470, 1471, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1482, 1483, 1486, 1494, 1497, 1498, 1500, 1501, 1502, 1503, 1505, 1507, 1509, 1510, 1511, 1512, 1513, 1516, 1517, 1519, 1523, 1525, 1526, 1531, 1532, 1533, 1535, 1536, 1537, 1539, 1540, 1542, 1543, 1544, 1545, 1550, 1552, 1554, 1557, 1560, 1562, 1563, 1565, 1566, 1567, 1568, 1569, 1570, 1573, 1578, 1579, 1580, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1596, 1597, 1599, 1600, 1601, 1602, 1606, 1607, 1608, 1611, 1615, 1618, 1619, 1620, 1621, 1624, 1625, 1627, 1628, 1630, 1633, 1636, 1639, 1641, 1647, 1648, 1650, 1651, 1655, 1657, 1658, 1662, 1665, 1666, 1671, 1672, 1674, 1675, 1679, 1684, 1685, 1689, 1693, 1694, 1695, 1696, 1699, 1701, 1702, 1703, 1705, 1706, 1710, 1712, 1714, 1717, 1718, 1719, 1720, 1721, 1724, 1726, 1727, 1728, 1731, 1732, 1733, 1734, 1735, 1738, 1743, 1744, 1746, 1749, 1751, 1755, 1757, 1761, 1765, 1766, 1767, 1773, 1775, 1777, 1778, 1779, 1784, 1786, 1790, 1794, 1796, 1800, 1810, 1813, 1815, 1818, 1828, 1831, 1833, 1835, 1836, 1837, 1838, 1842, 1845, 1848, 1850, 1852, 1854, 1856, 1857, 1858, 1859, 1860, 1861, 1864, 1865, 1866, 1869, 1871, 1873, 1874, 1876, 1878, 1879, 1881, 1882, 1883, 1885, 1887, 1888, 1889, 1890, 1891, 1895, 1899, 1902, 1903, 1905, 1906, 1908, 1910, 1911, 1912, 1914, 1916, 1919, 1920, 1921, 1925, 1926, 1928, 1930, 1931, 1932, 1936, 1939, 1940, 1943, 1944, 1948, 1953, 1955, 1960, 1961, 1966, 1970, 1971, 1972, 1975, 1976, 1977, 1979, 1981, 1983, 1984, 1986, 1987, 1990, 1993, 1994, 1995, 1996, 1997, 1999, 2000, 2001, 2008, 2014, 2017, 2019, 2023, 2027, 2029, 2030, 2031, 2033, 2037, 2038, 2039, 2040, 2041, 2042, 2044, 2045, 2046, 2047, 2050, 2053, 2056, 2057, 2058, 2060, 2062, 2064, 2066, 2070, 2072, 2073, 2074, 2075, 2079, 2080, 2085, 2088, 2089, 2091, 2092, 2093, 2094, 2095, 2097, 2100, 2102, 2106, 2113, 2114, 2115, 2118, 2119, 2122, 2124, 2125, 2129, 2130, 2135, 2139, 2140, 2142, 2143, 2144, 2145, 2146, 2149, 2151, 2154, 2155, 2156, 2157, 2160, 2161, 2166, 2167, 2168, 2170, 2172, 2173, 2174, 2175, 2176, 2177, 2179, 2181, 2183, 2185, 2186, 2187, 2191, 2192, 2196, 2199, 2203, 2207, 2209, 2210, 2212, 2218, 2225, 2231, 2232, 2233, 2235, 2240, 2241, 2244, 2245, 2253, 2255, 2258, 2259, 2260, 2261, 2262, 2267, 2268, 2269, 2274, 2275, 2276, 2277, 2278, 2280, 2281, 2282, 2284, 2288, 2289, 2290, 2294, 2295, 2296, 2297, 2298, 2303, 2309, 2310, 2315, 2321, 2327, 2329, 2334, 2336, 2339, 2351, 2352, 2353, 2354, 2358, 2359, 2360, 2364, 2368, 2369, 2370, 2374, 2376, 2377, 2380, 2385, 2389, 2391, 2396, 2397, 2400, 2402, 2403, 2408, 2409, 2413, 2414, 2417, 2418, 2420, 2423, 2424, 2425, 2428, 2429, 2430, 2432, 2437, 2446, 2449, 2453, 2458, 2461, 2462, 2463, 2465, 2469, 2470, 2474, 2475, 2477, 2480, 2482, 2484, 2486, 2490, 2494, 2503, 2505, 2508, 2509, 2511, 2513, 2514, 2516, 2517, 2518, 2519, 2525, 2527, 2529, 2530, 2534, 2535, 2536, 2538, 2541, 2547, 2548, 2551, 2552, 2553, 2554, 2555, 2558, 2561, 2562, 2563, 2565, 2566, 2569, 2573, 2574, 2575, 2577, 2579, 2583, 2587, 2592, 2593, 2594, 2598, 2600, 2602, 2606, 2607, 2608, 2609, 2610, 2612, 2613, 2614, 2616, 2618, 2620, 2622, 2623, 2625, 2627, 2629, 2630, 2633, 2634, 2636, 2637, 2641, 2643, 2644, 2649, 2653, 2656, 2658, 2659, 2660, 2662, 2668, 2670, 2672, 2673, 2674, 2675, 2676, 2677, 2680, 2681, 2682, 2683, 2684, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2697, 2699, 2700, 2701, 2703, 2705, 2707, 2709, 2713, 2716, 2718, 2720, 2722, 2725, 2726, 2727, 2729, 2733, 2734, 2736, 2738, 2741, 2742, 2744, 2745, 2748, 2753, 2759, 2763, 2765, 2767, 2769, 2773, 2775, 2776, 2779, 2786, 2787, 2791, 2792, 2794, 2798, 2800, 2802, 2803, 2807, 2808, 2810, 2811, 2813, 2814, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2825, 2826, 2827, 2828, 2829, 2831, 2832, 2833, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2848, 2850, 2857, 2861, 2865, 2867, 2870, 2874, 2876, 2877, 2879, 2880, 2881, 2882, 2883, 2884, 2886, 2887, 2888, 2889, 2891, 2893, 2894, 2895, 2896, 2899, 2900, 2902, 2905, 2906, 2921, 2923, 2924, 2925, 2926, 2927, 2929, 2932, 2933, 2936, 2942, 2943, 2945, 2947, 2948, 2952, 2954, 2955, 2957, 2959, 2960, 2964, 2967, 2968, 2973, 2974, 2975, 2980, 2984, 2985, 2986, 2989, 2990, 2992, 2993, 2996, 2997, 2998, 2999, 3000, 3001, 3004, 3005, 3006, 3010, 3011, 3012, 3013, 3014, 3017, 3020, 3022, 3023, 3027, 3028, 3030, 3031, 3033, 3034, 3037, 3039, 3045, 3046, 3048, 3049, 3051, 3055, 3056, 3057, 3059, 3060, 3064, 3067, 3069, 3071, 3072, 3073, 3074, 3075, 3077, 3078, 3081, 3083, 3084, 3085, 3087, 3089, 3092, 3095, 3098, 3099, 3102, 3103, 3104, 3107, 3108, 3110, 3111, 3112, 3113, 3114, 3116, 3117, 3118, 3119, 3120, 3122, 3125, 3126, 3128, 3130, 3133, 3134, 3136, 3138, 3139, 3141, 3142, 3143, 3144, 3145, 3149, 3150, 3152, 3153, 3156, 3157, 3158, 3159, 3161, 3163, 3164, 3166, 3168, 3170, 3174, 3179, 3180, 3181, 3186, 3187, 3189, 3194, 3195, 3199, 3200, 3204, 3211, 3212, 3215, 3217, 3220, 3230, 3232, 3233, 3234, 3235, 3241, 3242, 3243, 3244, 3247, 3249, 3252, 3255, 3259, 3260, 3263, 3265, 3269, 3270, 3271, 3281, 3283, 3284, 3287, 3293, 3295, 3298, 3299, 3302, 3305, 3307, 3308, 3312, 3313, 3316, 3317, 3318, 3322, 3324, 3325, 3329, 3332, 3337, 3338, 3340, 3341, 3342, 3344, 3347, 3348, 3350, 3351, 3352, 3354, 3356, 3357, 3368, 3369, 3371, 3373, 3376, 3378, 3382, 3386, 3387, 3389, 3390, 3395, 3396, 3397, 3399, 3400, 3406, 3408, 3410, 3412, 3417, 3418, 3420, 3422, 3423, 3427, 3428, 3429, 3431, 3432, 3435, 3438, 3441, 3442, 3444, 3447, 3448, 3451, 3455, 3456, 3457, 3458, 3464, 3469, 3471, 3478, 3480, 3482, 3484, 3485, 3488, 3489, 3493, 3497, 3500, 3505, 3508, 3509, 3511, 3515, 3519, 3520, 3521, 3523, 3524, 3529, 3530, 3537, 3541, 3544, 3545, 3549, 3550, 3551, 3554, 3555, 3556, 3559, 3560, 3561, 3565, 3567, 3570, 3571, 3578, 3582, 3589, 3590, 3600, 3606, 3607, 3610, 3612, 3617, 3619, 3620, 3621, 3622, 3623, 3624, 3625, 3627, 3628, 3629, 3632, 3637, 3638, 3639, 3641, 3642, 3645, 3649, 3651, 3652, 3656, 3657, 3658, 3660, 3661, 3662, 3663, 3667, 3672, 3673, 3675, 3677, 3678, 3680, 3684, 3690, 3693, 3695, 3697, 3698, 3699, 3700, 3701, 3702, 3706, 3707, 3709, 3710, 3715] 1609\n"
     ]
    }
   ],
   "source": [
    "fold_together = [i for i in fold126_2 if i in fold667_2 and i in fold159_2 and i in fold28_3 and i in fold28_1 and i in fold28_5 and i in fold28_8 and i in fold28_9 and i in fold251_8 and i in fold251_11 and i in fold251_16 and i in fold251_1 and i in fold251_2 and i in fold251_5 and i in fold251_18]\n",
    "print(fold_together, len(fold_together))\n",
    "\n",
    "lijst = fold_together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outliers = [85, 98, 152, 219, 231, 245, 264, 322, 362, 386, 415, 473]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-8ea2880b04ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlijst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mV_gif\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\ML_competition\\util\\vis4.py\u001b[0m in \u001b[0;36mvisualize\u001b[1;34m(pose, sample_id, frame_id, person_id, label)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0m_plot_bodypart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_face\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m70\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'blue'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'face'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0m_plot_bodypart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_hand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m95\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'green'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'l_hand'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_plot_bodypart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_hand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m116\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'magenta'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r_hand'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mlabel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'plots/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML_competition\\util\\vis4.py\u001b[0m in \u001b[0;36m_plot_bodypart\u001b[1;34m(adjacency, x, y, index_range, index_offset, color, pos)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0madjacency\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[1;31m# The y coordinate is flipped, because the origin lies in the top left corner in OpenPose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                 \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mindex_offset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mindex_offset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mindex_offset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mindex_offset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2811\u001b[0m     return gca().plot(\n\u001b[0;32m   2812\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m-> 2813\u001b[1;33m         is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1808\u001b[0m                         \u001b[1;34m\"the Matplotlib list!)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1810\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1611\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1612\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1613\u001b[0m             \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1614\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36madd_line\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   1891\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_artist_props\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1892\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1893\u001b[1;33m             \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1895\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_line_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mset_clip_path\u001b[1;34m(self, path, transform)\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRectangle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m                 self.clipbox = TransformedBbox(Bbox.unit(),\n\u001b[1;32m--> 686\u001b[1;33m                                                path.get_transform())\n\u001b[0m\u001b[0;32m    687\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clippath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\matplotlib\\patches.py\u001b[0m in \u001b[0;36mget_transform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPatch\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \"\"\"\n\u001b[1;32m--> 196\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_patch_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArtist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_data_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\matplotlib\\patches.py\u001b[0m in \u001b[0;36mget_patch_transform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_patch_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_patch_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rect_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\matplotlib\\patches.py\u001b[0m in \u001b[0;36m_update_patch_transform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[0mbbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_extents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m         \u001b[0mrot_trans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAffine2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m         \u001b[0mrot_trans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotate_deg_around\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mangle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rect_transform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBboxTransformTo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rect_transform\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mrot_trans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\matplotlib\\transforms.py\u001b[0m in \u001b[0;36mrotate_deg_around\u001b[1;34m(self, x, y, degrees)\u001b[0m\n\u001b[0;32m   2009\u001b[0m         \u001b[1;31m# Cast to float to avoid wraparound issues with uint8's\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2011\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotate_deg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2013\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\matplotlib\\transforms.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(self, tx, ty)\u001b[0m\n\u001b[0;32m   2021\u001b[0m         translate_mtx = np.array(\n\u001b[0;32m   2022\u001b[0m             [[1.0, 0.0, tx], [0.0, 1.0, ty], [0.0, 0.0, 1.0]], float)\n\u001b[1;32m-> 2023\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslate_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2024\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2025\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lijst = [5, 15, 25, 35]\n",
    "for sample in lijst:\n",
    "    for e in range(len(train_samples[sample])):\n",
    "        V_gif.visualize(train_samples[sample][e], sample, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file: 'C:\\Users\\main\\Documents\\ML_competition\\plots\\0\\508\\5.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-bcabacaf4d49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimageio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'plots/0/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mimageio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmimsave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.gif'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\imageio\\core\\functions.py\u001b[0m in \u001b[0;36mimread\u001b[1;34m(uri, format, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;31m# Get reader and read first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m     \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"i\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\imageio\\core\\functions.py\u001b[0m in \u001b[0;36mget_reader\u001b[1;34m(uri, format, mode, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;31m# Create request object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[0mrequest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;31m# Get format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\imageio\\core\\request.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, uri, mode, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;31m# Parse what was given\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;31m# Set extension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\imageio\\core\\request.py\u001b[0m in \u001b[0;36m_parse_uri\u001b[1;34m(self, uri)\u001b[0m\n\u001b[0;32m    271\u001b[0m                 \u001b[1;31m# Reading: check that the file exists (but is allowed a dir)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No such file: '%s'\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m                 \u001b[1;31m# Writing: check that the directory to write to does exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: No such file: 'C:\\Users\\main\\Documents\\ML_competition\\plots\\0\\508\\5.png'"
     ]
    }
   ],
   "source": [
    "for file in lijst:\n",
    "    images = []\n",
    "    new_path = 'gifs_gap_fixing/'\n",
    "    if not os.path.isdir(new_path): #make new path\n",
    "        os.mkdir(new_path)\n",
    "    for filename in range(len(train_samples[file])):\n",
    "        images.append(imageio.imread('plots/0/'+str(file)+'/'+str(filename)+'.png'))\n",
    "    imageio.mimsave(new_path+str(file)+'.gif', images)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose0 = all_samples[0][0]\n",
    "pose1 = all_samples[0][1]\n",
    "lengte = all_samples[519].shape[0]\n",
    "print(lengte)\n",
    "print(all_labels.shape[0])\n",
    "print(all_samples[0].shape[0])\n",
    "aantal_labels = 3718//18\n",
    "lijst = []\n",
    "som = 0\n",
    "h_label = 0\n",
    "for i in range(0, len(all_labels)): #aantal samples\n",
    "    if all_labels[i] not in lijst:\n",
    "        if (som > 20):\n",
    "            som = 0\n",
    "            h_label += 1\n",
    "            lijst.append(all_labels[i])\n",
    "            print(\"verhogen labels en toegevoegd\")\n",
    "        for e in range(all_samples[i].shape[0]): #aantal frames \n",
    "            V.visualize(all_samples[i][e], i, e, all_labels[i])\n",
    "        som += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import os\n",
    "\n",
    "print(\"imported\")\n",
    "paths = 'plots/'\n",
    "\n",
    "aantal_labels = 3718//18\n",
    "som = 0\n",
    "h_label = 0\n",
    "lijst = []\n",
    "for file in range(0, all_labels.shape[0]):\n",
    "    \n",
    "    if (som > 20):\n",
    "        som = 0\n",
    "        h_label += 1\n",
    "        print(\"verhogen labels\")\n",
    "    images = []\n",
    "    new_path = 'gifs/'+str(all_labels[file + h_label*aantal_labels])+'/'\n",
    "    if not os.path.isdir(new_path): #make new path\n",
    "        os.mkdir(new_path)\n",
    "    for filename in range(all_samples[file + h_label*aantal_labels].shape[0]):\n",
    "        images.append(imageio.imread(new_path+str(file + h_label*aantal_labels)+'/'+str(filename)+'.png'))\n",
    "    imageio.mimsave(new_path+str(file + h_label*aantal_labels)+'.gif', images)\n",
    "    som += 1\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: splitting the data set for validation and testing\n",
    "\n",
    "Machine learning models easily overfit to the training data. The result of overfitting is that the model doesn't generalise well, i.e., the model performance on unseen data is considerably worse than on the training data. For this reason, it is important to try and make a decent estimation of the performance on unseen data by splitting off a test set that is **not used at all** for building the model. \n",
    "\n",
    "We make the distinction between **training** and **tuning the hyperparameters**. In, e.g. , linear regression or logistic regression, training means optimising the weights for a given combination of model and training data. The **hyperparameters** are parameters at a higher level than the weights: they include things like which features (and how many) are used, the regularisation parameter values, or generally anything that affects the model complexity. \n",
    "\n",
    "The best way to look at this is that each setting of hyperparameters corresponds to a different model and from all these possible settings, we want to pick the best or at least a decent one. Again, a 'good' model is a model that performs well on **unseen** data, so we need to select our best hyperparameters based on data we haven't used for training. However, as soon as data has been used for making **any** decision about the model, it is no longer unseen, so we always need to keep some dat apart for evaluation the final quality of our model. \n",
    "\n",
    "Tuning many hyperparameters can also lead to overfitting. If we choose hyperparameters based on the test data, the score on this part of the data will again not be representative anymore for the score on completely unseen data. Therefore, we can not do the hyperparameter selection based on the test data. This is why we need to split the remaining data a second time into a **training set** and a **validation set**. \n",
    "\n",
    "If your test set is too small, your estimate of the generalisation performance will not be reliable, if your validation set is too small, your selection of hyperparameters will be sub-optimal because it's based on an unreliable estimate, and if you make both of these large, you don't have enough data left to train your model. If you have a lot of data, relative to the task and model complexity, this is no problem. However, in most cases, you don't have that amount of data. This is the reason why we often use **cross-validation**: we split the data and optimise the model multiple times and average the model performance accross all runs to get a better estimate of the true validation score. Obviously, the amount of averaging you can afford greatly depends on the training time of each model and the computational resources you have at your disposal. In most cases, k-fold cross-validation is used. Here, the training data is split into k equal parts (called **folds**) and the model is trained k times, each time using a different part as the validation set. The cross-validation score is the average model performance accross all k trained models, each evaluated on their validation set. Once the optimal hyperparameters are chosen, the model is retrained with **all** training data and then tested on the test set.\n",
    "\n",
    "In summary, the usual flow is:\n",
    "\n",
    "- **Split off a test set:** don't look at this data except for the evaluation of your final model\n",
    "- **Decide how many folds you need:** For large data sets, a single validation set is enough to give a decent estimate of the model error. However, for small data sets, the validation score can fluctuate a lot for different splits of the data, especially since we want to use as much data as possible for training, so validation sets will be small. For this reason, the same data set is often split multiple times into a train set and a validation set. For each **fold**, the model is trained and the validation error calculated. The best model parameters are chosen based on the average of the validation errors accross all the folds. One of the most common approaches is called **k-fold cross validation**. Here the dat is split into k equal (non-overlapping) parts and in each fold, a different part of the data is used for validation. \n",
    "- **Optimise the hyperparameters**\n",
    "- **Train your final model:** Once the optimal hyperparameters are found, these can be fixed and a final model can be trained using all the original training data, i.e., all data that was previously used for training **or** validation. This is often forgotten!\n",
    "- **Evaluate your final model:** use the test set you split off for evaluation. If you also need a more reliable test score (because you don't have enough data and have to keep your test set small), you can do nested cross validation by also considering multiple test sets, repeating the whole optimisation procedure for each test set and averaging the test scores. \n",
    "\n",
    "Although cross validation is well supported in sklearn and therefore sounds quite easy, this is also where many mistakes occur. In many real life situations, the data you have available for training is not really \"i.i.d.\". Instead, it comes from different subgroups, for instance, it may have been measured on different days or in different times of the year, originate from different test subjects, be collected by different enqueteurs, come from different customers or companies, ... and in most cases each of these subsets will have (hopefully slightly) different underlying distributions. Also, in most cases, you want your model to be robust against that, for instance, it should work with new patients or customers, or be useful to new companies. In that case, this is exactly what you need to evaluate during cross validation, by keeping entire subgroups within one fold and/or splitting off an entire subgroup in each test set. In addition, ideally, you are involved in the data collection: think about which differences may occur accross subgroups and really push having enough subgroups to allow your model to generalise beyond this. As we will see later in this course, the i.i.d. assumption also doesn't hold for time series. You can find more about cross-validation options on http://scikit-learn.org/stable/modules/cross_validation.html the sklearn documentation page.  \n",
    "\n",
    "Now back to our cancer data set. In this case, we have no additional information about possible subgroups in the data (e.g., the data might have been collected in different labs, by different lab assistants, or with slightly different measurement equipment). Therefore, the best we can do is randomly splitting the data into train and test sets and use cross-validation with random folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split off a test set of approximatly 25% of the data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_fraction = 0.25\n",
    "x_train, x_test, r_train, r_test = train_test_split(all_samples, all_labels, \n",
    "                                                    test_size = test_fraction, random_state=0)\n",
    "print('split complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training our first model\n",
    "\n",
    "Now we are ready to build our first model using logistic regression. Note that you can repeat all the steps below for any model (linear or nonlinear) that is available in the sklearn library. \n",
    "\n",
    "Mind that the score that is reported here is **accuracy**, i.e., the fraction of the samples for which the correct class is predicted by the model. As seen in class, what is actually optimised by logistic regression is logloss and other classification models optimise other loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a model instance \n",
    "# (this is a model object that can be trained, tuned and used)\n",
    "# Note that we are using all the default settings here!\n",
    "\n",
    "logreg0 = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Train the model on the training data\n",
    "for i in range(0, num_samples):\n",
    "    logreg0.fit(x_train[i], r_train[i])\n",
    "#logreg0.fit(x_train, r_train)\n",
    "\n",
    "# step 4: predict labels for test data \n",
    "\n",
    "pred0 = logreg0.predict(x_test)\n",
    "\n",
    "# step 4: calculate the accuracy of the model on the train set and on the test set\n",
    "\n",
    "train_score0 = logreg0.score(x_train, r_train)\n",
    "test_score0 = logreg0.score(x_test, r_test)\n",
    "\n",
    "print(\"Accuracy of first model: \",train_score0,\" (train), \",test_score0,\" (test)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
